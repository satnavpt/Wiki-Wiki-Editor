{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Liar Liar Fake News Detector",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satnavpt/Wiki-Wiki-Editor/blob/master/Liar_Liar_Fake_News_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8huogek1v8D"
      },
      "source": [
        "# **Fake News Classifier Using the Liar Liar Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofNGIHq02U3R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0RkrDbX2yWy"
      },
      "source": [
        "columns = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true',\t'false',\t'half_true',\t'mostly_true',\t'pants_on_fire',\t'venue']\n",
        "df_train = pd.read_table('liar_train.tsv', names = columns)    \n",
        "df_valid = pd.read_table('liar_valid.tsv', names = columns)\n",
        "df_test = pd.read_csv('liar_test.tsv', sep='\\t', names = columns) \n",
        "df = pd.concat([df_train, df_valid])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YHDGDv7_VU2"
      },
      "source": [
        "df.statement = df.statement.apply(lambda x: (re.sub(r'[^\\w ]+', \"\", x).lower()))\n",
        "df_test.statement = df_test.statement.apply(lambda x: (re.sub(r'[^\\w ]+', \"\", x).lower()))\n",
        "truth = {'false' : 0.0, 'half-true' : 0.0 ,'mostly-true' : 1.0, 'true' : 1.0, 'pants-fire' : 0.0, 'barely-true' : 0.0}\n",
        "training_data = [s.split() for s in df.statement.tolist()]\n",
        "training_results = df.label.apply(lambda x: truth[x])\n",
        "test_labels = df_test.label.apply(lambda x: truth[x]) #CALL test_labels instead??\n",
        "input_maxlen = 30\n",
        "test_data = [s.split() for s in df_test.statement.tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHas2w8Dj5dq"
      },
      "source": [
        "DIM = 20\n",
        "#The word2vec algorithm uses a neural network model to learn word associations and similarities by representing each unique word with a vector.\n",
        "wv2_model = gensim.models.Word2Vec(sentences = training_data, size = DIM, window = 6, min_count = 1, sample = 5e-3)\n",
        "#min_count - Ignores all words with total absolute frequency lower than this.\n",
        "  #As the dataset isn't large compared with the English language (12.8K statements), a low min_count is used.  \n",
        "#window  - The maximum distance between the current and predicted word within a sentence.\n",
        "  #A larger window gives results that are more semantic rather than syntatic, but requires a larger training dataset. \n",
        "#size - Dimensionality of the feature vectors. 300 works for a corpus of upto 60 million unique words, but the English language has an estimated 171k words in use and 47k obsolete words (Oxford English Dictionary)\n",
        "#sample - Determines how often higher-frequency words (such as \"the\") are trained. Values lower than 1e-5 tends to influence vector quality.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(training_data) #Train the tokenizer.\n",
        "vocab = tokenizer.word_index #Extact the currently recognised vocab\n",
        "vocab_size = len(vocab)\n",
        "training_data = tokenizer.texts_to_sequences(training_data)\n",
        "#Vectorizes the training data - turns each statement into either a sequence of integers (each of which is the index of a token in the vocab).\n",
        "training_data = pad_sequences(training_data, maxlen = input_maxlen, padding = 'post', truncating = 'post')\n",
        "#Padds or truncates a sentence's one-hot to a length of input_maxlen as the LSTM model created operates on fixed-size data points."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4VVJUN55b6H"
      },
      "source": [
        "def get_weight_matrix(model):\n",
        "  weight_matrix = np.zeros((vocab_size + 1, DIM))\n",
        "  for word, i in vocab.items():\n",
        "    weight_matrix[i] = model.wv[word] #Each word is assigned a weight acorrding to its word2vector representation. \n",
        "  return weight_matrix\n",
        "embedding_vectors = get_weight_matrix(wv2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjkzdejAeSiu"
      },
      "source": [
        "# Build the architecture of the model. \n",
        "model = tf.keras.Sequential([\n",
        "#A sequential model allows the model to be created layer-by-layer. Its input is an embedded sentence (of size sentence_len) and its output is a 0 - 1 float. \n",
        "    tf.keras.layers.Embedding(vocab_size + 1, output_dim = DIM, weights = [embedding_vectors], input_length = input_maxlen, trainable = False),\n",
        "#Create a 2D vector containing an embedding vector of size 100 for each word's one-hot.\n",
        "#Embedding vector -  Encodes the meaning of a word such that works that are closer together in the learned vector space are expected to be simmilar in meaning.\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "#Randomly set input units to 0 with a frequency of 0.1 and recale up the rest such that the sum over all inputs is unchanged. This helps prevent overfitting. \n",
        "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
        "#Downsamples the input representation by taking the maximum value over a spatial window of size pool_size. \n",
        "#This helps prevent over-fitting and reduces computational costs by reducing the number of parameters to learn. \n",
        "    tf.keras.layers.LSTM(units = 512, return_sequences = True)),\n",
        "#LSTM is a RNN that's effective in making predictions for long sequences of data such as sentences as it uses a memory cell to withhold past infromation for a longer time.\n",
        "    tf.keras.layers.LSTM(units = 512),\n",
        "    tf.keras.layers.Dropout(0.1),  \n",
        "    tf.keras.layers.Dense(512, activation = 'sigmoid'),\n",
        "#Dense feeds all outputs from the previous layer to all of its neurons, with each neuron providing one output (a misinformation prediction) via matrix-vector multiplication. \n",
        "    tf.keras.layers.Dropout(0.2),  \n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozzZOuJClDPG"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, epochs=2)\n",
        "print(\"Training Complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = tokenizer.texts_to_sequences(test_data)\n",
        "test_data = pad_sequences(test_data, maxlen = input_maxlen)"
      ],
      "metadata": {
        "id": "KtdiPJqjsJFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = (model.predict(test_data) >= 0.5).astype(int)\n",
        "accuracy_score(test_labels, test_pred, normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScSWLFupvvRd",
        "outputId": "81cb9c6c-e41d-471a-b3ad-2a609f2a7029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6456195737963694"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"First, you suffered through a Christmas without whipped cream. Now, you could be facing a bacon-less Super Bowlâ€”unless you're willing to pay a premium for a crispy slice of heaven. As of December, there were 17.8 million pounds of frozen pork belly, which is made into bacon, in US reserves, according to the Department of Agriculture.That might seem like a lot of bacon, but it's actually the lowest level since record-keeping began in January 1957. As the Cincinnati Enquirer puts it, there are literally not enough little piggies going to market. Pig farmers are actually producing more pigs than ever, says Rich Deaton of the Ohio Pork Council, yet our reserves are still depleting. The Enquirer notes foreign buyers may be responsible; 26% of pork produced in the U.S. is exported. Apart from being amazed that a bacon reserve exists, consumers appear terrified by the prospect of a bacon shortage. But Deaton says the pork industry will not run out of supply. Consumers should, however, expect to pay more for bacon at grocery stores. Prices have already spiked 20% last month. As for when we were most flush with frozen pork belly, that would be 1988, when there were 113.1 million pounds on hand.\"]\n",
        "x = pad_sequences(x, maxlen = input_maxlen)\n",
        "model.predict(x)"
      ],
      "metadata": {
        "id": "moz-dRNpxBnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_news_predict(sentence):\n",
        "  input_maxlen = 50\n",
        "  sentence_arr = [sentence] #The tokenizer requires an array of sentences. \n",
        "  sentence_arr = tokenizer.texts_to_sequences(sentence_arr) #Vectorize the sentence.\n",
        "  sentence_arr = pad_sequences(sa, maxlen = input_maxlen) #Pad the sentence so that each sentence in the inputted data is the same length.\n",
        "  return model.predict(sentence_arr)"
      ],
      "metadata": {
        "id": "WWLJrHhfrTa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data_detection(data):\n",
        "  sentences = sent_tokenize(data)\n",
        "  #Split data into sentences. \n",
        "  training_results = [fake_news_predict(s) for s in sentences]\n",
        "  #Get a misinformation prediction for each sentence. \n",
        "  return training_results"
      ],
      "metadata": {
        "id": "i_NiADQItXlx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
