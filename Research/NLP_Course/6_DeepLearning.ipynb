{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab3672e",
   "metadata": {},
   "source": [
    "Deep learning<br>\n",
    "- inputs will be values of features\n",
    "- multiply inputs by weights (weights initially start off as random)\n",
    "- these are passed into an acitivation function\n",
    "If the weights end up as zero then we may get stuck so introduce a bias term to fix this<br>\n",
    "\n",
    "z = $\\sum^n_{i=0}w_ix_i + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdd2c",
   "metadata": {},
   "source": [
    "Multiple Perceptrons Network\n",
    "- have several layers (hidden layers are ones that are between input and output)\n",
    "- 3 or more layers is called a \"deep network\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1adccdb",
   "metadata": {},
   "source": [
    "A good activation function is the sigmoid function (between 0 and 1)<br>\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852093cd",
   "metadata": {},
   "source": [
    "Another is tanh(z) (between -1 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79be7bc",
   "metadata": {},
   "source": [
    "Another is Rectified Linear Unit (ReLU), simply is max(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeffe2dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (4.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (60.6.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\nross\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\nross\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f71ab6",
   "metadata": {},
   "source": [
    "KERAS BASICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60b9d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072e62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f2e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target # our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773b6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to one hot coding so class 0 becomes [1, 0, 0] (there are 3 classes!)\n",
    "y = keras.utils.np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fe04d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7678f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # this also shuffles the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c62951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3bfbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()\n",
    "scaler_object.fit(X_train) # fit on training features\n",
    "\n",
    "scaled_X_train = scaler_object.transform(X_train) # ensures data is between 0 and 1 (divides by max of training set)\n",
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad61570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca67cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu')) # 8 is the number of neurons\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) # 3 neurons for our 3 outputs such as [0.3, 0.2, 0.5]\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # our outputs are categories\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8629c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 - 0s - loss: 0.0739 - accuracy: 0.9714 - 7ms/epoch - 2ms/step\n",
      "Epoch 2/150\n",
      "4/4 - 0s - loss: 0.0746 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 3/150\n",
      "4/4 - 0s - loss: 0.0740 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 4/150\n",
      "4/4 - 0s - loss: 0.0734 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 5/150\n",
      "4/4 - 0s - loss: 0.0731 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 6/150\n",
      "4/4 - 0s - loss: 0.0724 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 7/150\n",
      "4/4 - 0s - loss: 0.0719 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 8/150\n",
      "4/4 - 0s - loss: 0.0721 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 9/150\n",
      "4/4 - 0s - loss: 0.0716 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 10/150\n",
      "4/4 - 0s - loss: 0.0716 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 11/150\n",
      "4/4 - 0s - loss: 0.0715 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 12/150\n",
      "4/4 - 0s - loss: 0.0715 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 13/150\n",
      "4/4 - 0s - loss: 0.0718 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 14/150\n",
      "4/4 - 0s - loss: 0.0716 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 15/150\n",
      "4/4 - 0s - loss: 0.0712 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 16/150\n",
      "4/4 - 0s - loss: 0.0696 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 17/150\n",
      "4/4 - 0s - loss: 0.0697 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 18/150\n",
      "4/4 - 0s - loss: 0.0695 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 19/150\n",
      "4/4 - 0s - loss: 0.0695 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 20/150\n",
      "4/4 - 0s - loss: 0.0696 - accuracy: 0.9810 - 6ms/epoch - 1ms/step\n",
      "Epoch 21/150\n",
      "4/4 - 0s - loss: 0.0691 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 22/150\n",
      "4/4 - 0s - loss: 0.0690 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 23/150\n",
      "4/4 - 0s - loss: 0.0685 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 24/150\n",
      "4/4 - 0s - loss: 0.0679 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 25/150\n",
      "4/4 - 0s - loss: 0.0678 - accuracy: 0.9714 - 7ms/epoch - 2ms/step\n",
      "Epoch 26/150\n",
      "4/4 - 0s - loss: 0.0678 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 27/150\n",
      "4/4 - 0s - loss: 0.0689 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 28/150\n",
      "4/4 - 0s - loss: 0.0671 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 29/150\n",
      "4/4 - 0s - loss: 0.0672 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 30/150\n",
      "4/4 - 0s - loss: 0.0671 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 31/150\n",
      "4/4 - 0s - loss: 0.0666 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 32/150\n",
      "4/4 - 0s - loss: 0.0662 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 33/150\n",
      "4/4 - 0s - loss: 0.0662 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 34/150\n",
      "4/4 - 0s - loss: 0.0662 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 35/150\n",
      "4/4 - 0s - loss: 0.0657 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 36/150\n",
      "4/4 - 0s - loss: 0.0656 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 37/150\n",
      "4/4 - 0s - loss: 0.0653 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 38/150\n",
      "4/4 - 0s - loss: 0.0652 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 39/150\n",
      "4/4 - 0s - loss: 0.0650 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 40/150\n",
      "4/4 - 0s - loss: 0.0648 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 41/150\n",
      "4/4 - 0s - loss: 0.0644 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 42/150\n",
      "4/4 - 0s - loss: 0.0644 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 43/150\n",
      "4/4 - 0s - loss: 0.0643 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 44/150\n",
      "4/4 - 0s - loss: 0.0644 - accuracy: 0.9810 - 4ms/epoch - 999us/step\n",
      "Epoch 45/150\n",
      "4/4 - 0s - loss: 0.0640 - accuracy: 0.9810 - 4ms/epoch - 1ms/step\n",
      "Epoch 46/150\n",
      "4/4 - 0s - loss: 0.0637 - accuracy: 0.9810 - 6ms/epoch - 1ms/step\n",
      "Epoch 47/150\n",
      "4/4 - 0s - loss: 0.0641 - accuracy: 0.9714 - 4ms/epoch - 1000us/step\n",
      "Epoch 48/150\n",
      "4/4 - 0s - loss: 0.0644 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 49/150\n",
      "4/4 - 0s - loss: 0.0635 - accuracy: 0.9714 - 4ms/epoch - 1000us/step\n",
      "Epoch 50/150\n",
      "4/4 - 0s - loss: 0.0631 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 51/150\n",
      "4/4 - 0s - loss: 0.0627 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 52/150\n",
      "4/4 - 0s - loss: 0.0626 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 53/150\n",
      "4/4 - 0s - loss: 0.0624 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 54/150\n",
      "4/4 - 0s - loss: 0.0623 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 55/150\n",
      "4/4 - 0s - loss: 0.0621 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 56/150\n",
      "4/4 - 0s - loss: 0.0617 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 57/150\n",
      "4/4 - 0s - loss: 0.0615 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 58/150\n",
      "4/4 - 0s - loss: 0.0622 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 59/150\n",
      "4/4 - 0s - loss: 0.0622 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 60/150\n",
      "4/4 - 0s - loss: 0.0615 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 61/150\n",
      "4/4 - 0s - loss: 0.0611 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 62/150\n",
      "4/4 - 0s - loss: 0.0609 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 63/150\n",
      "4/4 - 0s - loss: 0.0606 - accuracy: 0.9714 - 7ms/epoch - 2ms/step\n",
      "Epoch 64/150\n",
      "4/4 - 0s - loss: 0.0606 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 65/150\n",
      "4/4 - 0s - loss: 0.0609 - accuracy: 0.9810 - 7ms/epoch - 2ms/step\n",
      "Epoch 66/150\n",
      "4/4 - 0s - loss: 0.0604 - accuracy: 0.9810 - 6ms/epoch - 2ms/step\n",
      "Epoch 67/150\n",
      "4/4 - 0s - loss: 0.0601 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 68/150\n",
      "4/4 - 0s - loss: 0.0600 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 69/150\n",
      "4/4 - 0s - loss: 0.0598 - accuracy: 0.9714 - 7ms/epoch - 2ms/step\n",
      "Epoch 70/150\n",
      "4/4 - 0s - loss: 0.0597 - accuracy: 0.9714 - 8ms/epoch - 2ms/step\n",
      "Epoch 71/150\n",
      "4/4 - 0s - loss: 0.0595 - accuracy: 0.9714 - 8ms/epoch - 2ms/step\n",
      "Epoch 72/150\n",
      "4/4 - 0s - loss: 0.0596 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 73/150\n",
      "4/4 - 0s - loss: 0.0603 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 74/150\n",
      "4/4 - 0s - loss: 0.0601 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 75/150\n",
      "4/4 - 0s - loss: 0.0601 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 76/150\n",
      "4/4 - 0s - loss: 0.0592 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 77/150\n",
      "4/4 - 0s - loss: 0.0591 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 78/150\n",
      "4/4 - 0s - loss: 0.0585 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 79/150\n",
      "4/4 - 0s - loss: 0.0584 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 80/150\n",
      "4/4 - 0s - loss: 0.0584 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 81/150\n",
      "4/4 - 0s - loss: 0.0584 - accuracy: 0.9714 - 4ms/epoch - 999us/step\n",
      "Epoch 82/150\n",
      "4/4 - 0s - loss: 0.0580 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 83/150\n",
      "4/4 - 0s - loss: 0.0567 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 84/150\n",
      "4/4 - 0s - loss: 0.0586 - accuracy: 0.9810 - 6ms/epoch - 1ms/step\n",
      "Epoch 85/150\n",
      "4/4 - 0s - loss: 0.0597 - accuracy: 0.9905 - 6ms/epoch - 1ms/step\n",
      "Epoch 86/150\n",
      "4/4 - 0s - loss: 0.0588 - accuracy: 0.9905 - 6ms/epoch - 1ms/step\n",
      "Epoch 87/150\n",
      "4/4 - 0s - loss: 0.0578 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 88/150\n",
      "4/4 - 0s - loss: 0.0575 - accuracy: 0.9810 - 4ms/epoch - 1000us/step\n",
      "Epoch 89/150\n",
      "4/4 - 0s - loss: 0.0571 - accuracy: 0.9810 - 8ms/epoch - 2ms/step\n",
      "Epoch 90/150\n",
      "4/4 - 0s - loss: 0.0568 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 91/150\n",
      "4/4 - 0s - loss: 0.0566 - accuracy: 0.9714 - 7ms/epoch - 2ms/step\n",
      "Epoch 92/150\n",
      "4/4 - 0s - loss: 0.0569 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 93/150\n",
      "4/4 - 0s - loss: 0.0563 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 94/150\n",
      "4/4 - 0s - loss: 0.0566 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 95/150\n",
      "4/4 - 0s - loss: 0.0564 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 96/150\n",
      "4/4 - 0s - loss: 0.0562 - accuracy: 0.9810 - 6ms/epoch - 1ms/step\n",
      "Epoch 97/150\n",
      "4/4 - 0s - loss: 0.0558 - accuracy: 0.9810 - 6ms/epoch - 2ms/step\n",
      "Epoch 98/150\n",
      "4/4 - 0s - loss: 0.0558 - accuracy: 0.9810 - 6ms/epoch - 2ms/step\n",
      "Epoch 99/150\n",
      "4/4 - 0s - loss: 0.0556 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 100/150\n",
      "4/4 - 0s - loss: 0.0557 - accuracy: 0.9810 - 6ms/epoch - 2ms/step\n",
      "Epoch 101/150\n",
      "4/4 - 0s - loss: 0.0553 - accuracy: 0.9714 - 6ms/epoch - 1ms/step\n",
      "Epoch 102/150\n",
      "4/4 - 0s - loss: 0.0551 - accuracy: 0.9714 - 6ms/epoch - 2ms/step\n",
      "Epoch 103/150\n",
      "4/4 - 0s - loss: 0.0550 - accuracy: 0.9714 - 4ms/epoch - 1000us/step\n",
      "Epoch 104/150\n",
      "4/4 - 0s - loss: 0.0549 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/150\n",
      "4/4 - 0s - loss: 0.0548 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 106/150\n",
      "4/4 - 0s - loss: 0.0549 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 107/150\n",
      "4/4 - 0s - loss: 0.0547 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 108/150\n",
      "4/4 - 0s - loss: 0.0549 - accuracy: 0.9714 - 4ms/epoch - 1000us/step\n",
      "Epoch 109/150\n",
      "4/4 - 0s - loss: 0.0545 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 110/150\n",
      "4/4 - 0s - loss: 0.0545 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 111/150\n",
      "4/4 - 0s - loss: 0.0548 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 112/150\n",
      "4/4 - 0s - loss: 0.0546 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 113/150\n",
      "4/4 - 0s - loss: 0.0543 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 114/150\n",
      "4/4 - 0s - loss: 0.0545 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 115/150\n",
      "4/4 - 0s - loss: 0.0542 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 116/150\n",
      "4/4 - 0s - loss: 0.0542 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 117/150\n",
      "4/4 - 0s - loss: 0.0545 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 118/150\n",
      "4/4 - 0s - loss: 0.0538 - accuracy: 0.9714 - 4ms/epoch - 1ms/step\n",
      "Epoch 119/150\n",
      "4/4 - 0s - loss: 0.0536 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 120/150\n",
      "4/4 - 0s - loss: 0.0529 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 121/150\n",
      "4/4 - 0s - loss: 0.0530 - accuracy: 0.9905 - 4ms/epoch - 1000us/step\n",
      "Epoch 122/150\n",
      "4/4 - 0s - loss: 0.0532 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 123/150\n",
      "4/4 - 0s - loss: 0.0536 - accuracy: 0.9905 - 4ms/epoch - 1ms/step\n",
      "Epoch 124/150\n",
      "4/4 - 0s - loss: 0.0539 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 125/150\n",
      "4/4 - 0s - loss: 0.0541 - accuracy: 0.9905 - 6ms/epoch - 2ms/step\n",
      "Epoch 126/150\n",
      "4/4 - 0s - loss: 0.0528 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 127/150\n",
      "4/4 - 0s - loss: 0.0522 - accuracy: 0.9905 - 6ms/epoch - 1ms/step\n",
      "Epoch 128/150\n",
      "4/4 - 0s - loss: 0.0519 - accuracy: 0.9905 - 4ms/epoch - 1ms/step\n",
      "Epoch 129/150\n",
      "4/4 - 0s - loss: 0.0522 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 130/150\n",
      "4/4 - 0s - loss: 0.0521 - accuracy: 0.9714 - 5ms/epoch - 1ms/step\n",
      "Epoch 131/150\n",
      "4/4 - 0s - loss: 0.0527 - accuracy: 0.9714 - 4ms/epoch - 1000us/step\n",
      "Epoch 132/150\n",
      "4/4 - 0s - loss: 0.0519 - accuracy: 0.9810 - 4ms/epoch - 1000us/step\n",
      "Epoch 133/150\n",
      "4/4 - 0s - loss: 0.0516 - accuracy: 0.9810 - 4ms/epoch - 1ms/step\n",
      "Epoch 134/150\n",
      "4/4 - 0s - loss: 0.0513 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 135/150\n",
      "4/4 - 0s - loss: 0.0516 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 136/150\n",
      "4/4 - 0s - loss: 0.0514 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 137/150\n",
      "4/4 - 0s - loss: 0.0511 - accuracy: 0.9905 - 6ms/epoch - 2ms/step\n",
      "Epoch 138/150\n",
      "4/4 - 0s - loss: 0.0506 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 139/150\n",
      "4/4 - 0s - loss: 0.0507 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 140/150\n",
      "4/4 - 0s - loss: 0.0510 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 141/150\n",
      "4/4 - 0s - loss: 0.0511 - accuracy: 0.9810 - 5ms/epoch - 1ms/step\n",
      "Epoch 142/150\n",
      "4/4 - 0s - loss: 0.0500 - accuracy: 0.9905 - 4ms/epoch - 999us/step\n",
      "Epoch 143/150\n",
      "4/4 - 0s - loss: 0.0501 - accuracy: 0.9905 - 4ms/epoch - 1ms/step\n",
      "Epoch 144/150\n",
      "4/4 - 0s - loss: 0.0495 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 145/150\n",
      "4/4 - 0s - loss: 0.0493 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 146/150\n",
      "4/4 - 0s - loss: 0.0492 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 147/150\n",
      "4/4 - 0s - loss: 0.0492 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 148/150\n",
      "4/4 - 0s - loss: 0.0493 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 149/150\n",
      "4/4 - 0s - loss: 0.0485 - accuracy: 0.9905 - 5ms/epoch - 1ms/step\n",
      "Epoch 150/150\n",
      "4/4 - 0s - loss: 0.0484 - accuracy: 0.9905 - 4ms/epoch - 1000us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18695492c40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X_train, y_train, epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f51cfb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 1, 2,\n",
       "       2, 2, 1, 1, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(scaled_X_test), axis=-1) # print out the class (not one hot coded) \n",
    "#(just using .predict will give probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "894fc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(scaled_X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "200c7da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 2, 2,\n",
       "       2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1, 0, 2, 2, 0, 2,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67bf4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "102e8a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0],\n",
       "       [ 0, 11,  2],\n",
       "       [ 0,  1, 17]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef2828b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.92      0.85      0.88        13\n",
      "           2       0.89      0.94      0.92        18\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.93      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df462569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test.argmax(axis=1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c16b3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.h5') # allows us to save the model, will override any file with same name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "134ea1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f0b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b347a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62083da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "731d3379",
   "metadata": {},
   "source": [
    "RECURRENT NEURAL NETWORKS<br>\n",
    "- sends output back to itself!\n",
    "- two sets of input, inputs from previous timestep and from current timestep\n",
    "- cells that are a function of inputs from previous time steps are also known as memory cells\n",
    "- RNNs are flexible in their inputs and outputs, for both sequences and single vector values\n",
    "- e.g. we could feed a sequence in and get a sequence out, each element is put in one by one\n",
    "- e.g. can have a sequence input and only get a vector out once every element in sequence has been inputted\n",
    "- e.g. could feed in a single vector and then getting a sequences out \n",
    "\n",
    "If done on a lot of inputs, the RNN will begin to \"forget\" the first inputs.<br>\n",
    "\n",
    "The LSTM (Long Short Term Memory) cell was created to help address the issue described above. LSTM cells have another input and output for the cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b1ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a3135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b006e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d34f94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623 # ensure it can read all of the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74224e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation etc (don't want to train on punctuation)\n",
    "def seperate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--,*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "05dba8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a99748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = seperate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3985fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in 25 words and try to predict the next word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e1705d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "\n",
    "text_seq = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_seq.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "41aa76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5344e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7f18ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_seq) # now the text is a sequence of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cce7d0d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'a', 3: '.', 4: 'and', 5: 'of', 6: 'i', 7: 'to', 8: 'in', 9: 'it', 10: 'that', 11: 'he', 12: 'his', 13: 'was', 14: 'but', 15: 'me', 16: 'with', 17: 'as', 18: 'at', 19: 'this', 20: 'you', 21: 'is', 22: 'all', 23: 'for', 24: 'my', 25: 'on', 26: 'be', 27: \"'s\", 28: 'not', 29: 'from', 30: 'there', 31: 'one', 32: 'up', 33: 'what', 34: 'him', 35: 'so', 36: 'bed', 37: 'now', 38: 'about', 39: 'no', 40: 'into', 41: 'by', 42: 'were', 43: 'out', 44: 'or', 45: 'harpooneer', 46: 'had', 47: 'then', 48: 'have', 49: 'an', 50: 'upon', 51: 'little', 52: 'some', 53: 'old', 54: 'like', 55: 'if', 56: 'they', 57: 'would', 58: 'do', 59: 'over', 60: 'landlord', 61: 'thought', 62: 'room', 63: 'when', 64: 'could', 65: \"n't\", 66: 'night', 67: 'here', 68: 'head', 69: 'such', 70: 'which', 71: 'man', 72: 'did', 73: 'sea', 74: 'time', 75: 'other', 76: 'very', 77: 'go', 78: 'these', 79: 'more', 80: 'though', 81: 'first', 82: 'sort', 83: 'said', 84: 'last', 85: 'down', 86: 'most', 87: 'been', 88: 'never', 89: 'your', 90: 'them', 91: 'must', 92: 'tell', 93: 'much', 94: 'good', 95: 'see', 96: 'off', 97: 'myself', 98: 'are', 99: 'yet', 100: 'sleep', 101: 'who', 102: 'seemed', 103: 'light', 104: 'way', 105: 'their', 106: 'just', 107: 'being', 108: 'than', 109: 'place', 110: 'queequeg', 111: 'great', 112: 'long', 113: 'before', 114: 'get', 115: 'round', 116: 'where', 117: 'still', 118: 'any', 119: 'too', 120: 'only', 121: 'door', 122: 'can', 123: 'himself', 124: 'heads', 125: 'come', 126: 'ever', 127: 'two', 128: 'enough', 129: 'made', 130: 'how', 131: 'hand', 132: 'same', 133: 'looking', 134: 'something', 135: 'may', 136: \"'\", 137: 'almost', 138: 'say', 139: 'should', 140: 'side', 141: 'why', 142: 'own', 143: 'we', 144: 'new', 145: 'again', 146: 'came', 147: 'arm', 148: 'house', 149: 'away', 150: 'might', 151: 'nothing', 152: 'take', 153: 'towards', 154: 'will', 155: 'under', 156: 'going', 157: 'make', 158: 'whale', 159: 'stood', 160: 'boots', 161: 'ye', 162: 'back', 163: \"'ll\", 164: 'tomahawk', 165: 'part', 166: 'world', 167: 'soon', 168: 'water', 169: 'against', 170: 'those', 171: 'between', 172: 'after', 173: 'whaling', 174: 'lay', 175: 'took', 176: 'half', 177: 'began', 178: 'face', 179: 'streets', 180: 'land', 181: 'better', 182: 'once', 183: 'voyage', 184: 'give', 185: 'rather', 186: 'well', 187: 'however', 188: 'else', 189: 'heard', 190: 'put', 191: 'stop', 192: 'dark', 193: 'went', 194: 'black', 195: 'window', 196: 'cannibal', 197: 'fire', 198: 'every', 199: 'ship', 200: 'stand', 201: 'strange', 202: 'without', 203: 'feet', 204: 'whether', 205: 'because', 206: 'eyes', 207: 'think', 208: 'thinks', 209: 'idea', 210: 'bag', 211: 'nantucket', 212: 'late', 213: 'cold', 214: 'our', 215: 'found', 216: 'full', 217: 'morning', 218: 'sleeping', 219: 'got', 220: 'mind', 221: 'her', 222: 'right', 223: 'its', 224: 'look', 225: 'town', 226: 'south', 227: 'does', 228: 'let', 229: 'set', 230: 'yourself', 231: 'image', 232: 'saw', 233: 'am', 234: 'besides', 235: 'sailor', 236: 'seas', 237: 'rolled', 238: 'till', 239: 'day', 240: 'sign', 241: 'looked', 242: 'hard', 243: 'moment', 244: 'corner', 245: 'entry', 246: 'four', 247: 'wall', 248: 'savage', 249: 'table', 250: 'indeed', 251: 'bench', 252: 'chest', 253: 'while', 254: 'stranger', 255: 'possible', 256: 'feeling', 257: 'floor', 258: 'squares', 259: 'hat', 260: 'particular', 261: 'having', 262: 'years', 263: 'harpoon', 264: 'ishmael', 265: 'whenever', 266: 'mouth', 267: 'high', 268: 'knew', 269: 'men', 270: 'hours', 271: 'green', 272: 'bit', 273: 'within', 274: 'picture', 275: 'told', 276: 'story', 277: 'mean', 278: 'speak', 279: 'order', 280: 'making', 281: 'even', 282: 'perhaps', 283: 'things', 284: 'answer', 285: 'parts', 286: 'wild', 287: 'reason', 288: 'young', 289: 'craft', 290: 'business', 291: 'dead', 292: 'another', 293: 'middle', 294: 'sure', 295: 'candle', 296: 'presently', 297: 'low', 298: 'turned', 299: 'teeth', 300: 'dim', 301: 'euroclydon', 302: 'kept', 303: 'glass', 304: 'afterwards', 305: 'large', 306: 'three', 307: 'telling', 308: 'getting', 309: 'small', 310: 'next', 311: 'seeing', 312: 'sell', 313: 'felt', 314: 'sun', 315: 'money', 316: 'sail', 317: 'coffin', 318: 'especially', 319: 'street', 320: 'city', 321: 'few', 322: 'previous', 323: 'sight', 324: 'days', 325: 'straight', 326: 'nigh', 327: 'legs', 328: 'try', 329: 'yes', 330: 'unless', 331: 'poor', 332: 'coat', 333: 'passenger', 334: 'taking', 335: 'true', 336: 'thing', 337: 'ai', 338: 'always', 339: 'us', 340: 'really', 341: 'marvellous', 342: 'heaven', 343: 'air', 344: 'far', 345: 'second', 346: 'many', 347: 'has', 348: 'unaccountable', 349: 'grand', 350: 'jolly', 351: 'open', 352: 'shirt', 353: 'cape', 354: 'bedford', 355: 'fine', 356: 'further', 357: 'ice', 358: 'frost', 359: 'foot', 360: 'wide', 361: 'white', 362: 'tall', 363: 'i.', 364: 'wooden', 365: 'worse', 366: 'death', 367: 'mine', 368: 'lazarus', 369: 'keep', 370: 'along', 371: 'hung', 372: 'throwing', 373: 'centre', 374: 'rest', 375: 'fact', 376: 'hair', 377: 'broken', 378: 'kill', 379: 'through', 380: 'chimney', 381: 'fancy', 382: 'bar', 383: 'trying', 384: 'dumplings', 385: 'heavens', 386: 'manner', 387: 'devil', 388: 'together', 389: 'seen', 390: 'deal', 391: 'know', 392: 'skin', 393: 'ca', 394: 'shavings', 395: 'peddling', 396: 'sunday', 397: 'counterpane', 398: 'mat', 399: 'christian', 400: 'commenced', 401: 'thinking', 402: 'similar', 403: 'afraid', 404: 'length', 405: 'idol', 406: 'e', 407: 'sabbee', 408: 'waking', 409: 'ago', 410: 'find', 411: 'damp', 412: 'soul', 413: 'strong', 414: 'account', 415: 'sword', 416: 'quietly', 417: 'degree', 418: 'left', 419: 'around', 420: 'fixed', 421: 'ships', 422: 'miles', 423: 'country', 424: 'stream', 425: 'lead', 426: 'american', 427: 'artist', 428: 'each', 429: 'goes', 430: 'deep', 431: 'distant', 432: 'winds', 433: 'blue', 434: 'among', 435: 'suddenly', 436: 'feel', 437: 'meaning', 438: 'phantom', 439: 'life', 440: 'passengers', 441: 'nor', 442: 'kind', 443: 'quite', 444: 'care', 445: 'board', 446: 'somehow', 447: 'broiled', 448: 'mast', 449: 'sense', 450: 'knowing', 451: 'either', 452: 'passed', 453: 'hands', 454: 'paying', 455: 'pay', 456: 'penny', 457: 'sailors', 458: 'exactly', 459: 'short', 460: 'easy', 461: 'portentous', 462: 'island', 463: 'nameless', 464: 'sounds', 465: 'since', 466: 'snow', 467: 'saturday', 468: 'matter', 469: 'red', 470: 'partly', 471: 'ere', 472: 'became', 473: 'meanwhile', 474: 'pocket', 475: 'darkness', 476: 'fish', 477: 'inn', 478: 'watch', 479: 'broad', 480: 'entering', 481: 'ha', 482: 'ashes', 483: 'opened', 484: 'spouter', 485: 'name', 486: 'suppose', 487: 'quiet', 488: 'best', 489: 'tempestuous', 490: 'says', 491: 'thou', 492: 'both', 493: 'occurred', 494: 'dives', 495: 'holding', 496: 'frozen', 497: 'altogether', 498: 'plain', 499: 'whom', 500: 'clean', 501: 'human', 502: 'entered', 503: 'wrinkled', 504: 'shelf', 505: 'jonah', 506: 'blanket', 507: \"goin'\", 508: 'bitter', 509: 'supper', 510: 'sat', 511: 'settle', 512: 'chap', 513: 'help', 514: 'spend', 515: 'landed', 516: 'standing', 517: 'held', 518: 'somewhat', 519: 'sober', 520: 'whole', 521: 'dam', 522: 'brown', 523: 'bulkington', 524: \"o'clock\", 525: 'none', 526: 'coming', 527: \"'ve\", 528: 'wait', 529: 'plane', 530: 'saying', 531: 'grinning', 532: 'placed', 533: 'shouted', 534: 'bedfellow', 535: 'zealand', 536: 'sal', 537: 'wash', 538: 'thrown', 539: 'purplish', 540: 'turn', 541: 'completely', 542: 'fear', 543: 'grego', 544: 'baby', 545: 'slowly', 546: 'civilized', 547: 'purse', 548: 'monkey', 549: 'involuntarily', 550: 'pausing', 551: 'warehouses', 552: 'requires', 553: 'people', 554: 'nearly', 555: 'ocean', 556: 'indian', 557: 'waterward', 558: 'battery', 559: 'noble', 560: 'washed', 561: 'crowds', 562: 'sabbath', 563: 'afternoon', 564: 'thence', 565: 'silent', 566: 'thousands', 567: 'reveries', 568: 'leaning', 569: 'seated', 570: 'bulwarks', 571: 'aloft', 572: 'week', 573: 'plaster', 574: 'gone', 575: 'bound', 576: 'content', 577: 'yonder', 578: 'falling', 579: 'north', 580: 'please', 581: 'ten', 582: 'leaves', 583: 'magic', 584: 'plunged', 585: 'metaphysical', 586: 'chief', 587: 'trunk', 588: 'meadow', 589: 'smoke', 590: 'reaching', 591: 'hill', 592: 'thus', 593: 'pine', 594: 'sighs', 595: 'shepherd', 596: 'june', 597: 'scores', 598: 'thousand', 599: 'sadly', 600: 'robust', 601: 'healthy', 602: 'boy', 603: 'crazy', 604: 'hold', 605: 'holy', 606: 'brother', 607: 'ourselves', 608: 'begin', 609: 'grow', 610: 'inferred', 611: 'needs', 612: \"don't\", 613: 'themselves', 614: 'commodore', 615: 'captain', 616: 'cook', 617: 'glory', 618: 'whatsoever', 619: 'confess', 620: 'officer', 621: 'respectfully', 622: 'horse', 623: 'huge', 624: 'houses', 625: 'forecastle', 626: 'jump', 627: 'spar', 628: 'particularly', 629: 'putting', 630: 'tar', 631: 'schoolmaster', 632: 'boys', 633: 'transition', 634: 'grin', 635: 'bear', 636: 'hunks', 637: 'sweep', 638: 'weighed', 639: 'anything', 640: 'less', 641: 'thump', 642: 'point', 643: 'view', 644: 'single', 645: 'difference', 646: 'act', 647: 'uncomfortable', 648: 'compare', 649: 'earthly', 650: 'enter', 651: 'deck', 652: 'quarter', 653: 'leaders', 654: 'smelt', 655: 'fates', 656: 'doubtless', 657: 'formed', 658: 'run', 659: 'stage', 660: 'shabby', 661: 'others', 662: 'circumstances', 663: 'motives', 664: 'various', 665: 'mysterious', 666: 'curiosity', 667: 'tormented', 668: 'everlasting', 669: 'wonder', 670: 'purpose', 671: 'floated', 672: 'stuffed', 673: 'horn', 674: 'arrived', 675: 'offer', 676: 'following', 677: 'embark', 678: 'pleased', 679: 'original', 680: 'stranded', 681: 'leviathan', 682: 'whales', 683: 'nay', 684: 'dismal', 685: 'wherever', 686: 'dreary', 687: 'crossed', 688: 'expensive', 689: 'bright', 690: 'windows', 691: 'packed', 692: 'inches', 693: 'thick', 694: 'hear', 695: 'tinkling', 696: 'glasses', 697: 'blackness', 698: 'moving', 699: 'hour', 700: 'proved', 701: 'meant', 702: 'public', 703: 'box', 704: 'flying', 705: 'harpoons', 706: 'trap', 707: 'loud', 708: 'voice', 709: 'sitting', 710: 'beyond', 711: 'negro', 712: 'creaking', 713: 'swinging', 714: 'painting', 715: 'representing', 716: 'connexion', 717: 'peter', 718: 'itself', 719: 'carted', 720: 'burnt', 721: 'spot', 722: 'queer', 723: 'gable', 724: 'ended', 725: 'sharp', 726: 'wind', 727: 'howling', 728: 'nevertheless', 729: 'called', 730: 'outside', 731: 'passage', 732: 'body', 733: 'curbstone', 734: 'corn', 735: 'pooh', 736: 'northern', 737: 'lights', 738: 'summer', 739: 'lengthwise', 740: 'gods', 741: 'lie', 742: 'plenty', 743: 'study', 744: 'series', 745: 'arrive', 746: 'shadows', 747: 'dint', 748: 'conclusion', 749: 'confounded', 750: 'limber', 751: 'truly', 752: 'drive', 753: 'unimaginable', 754: 'midnight', 755: 'unnatural', 756: 'breaking', 757: 'design', 758: 'opposite', 759: 'monstrous', 760: 'knots', 761: 'vast', 762: 'handle', 763: 'wondered', 764: 'mixed', 765: 'deformed', 766: 'flung', 767: 'iron', 768: 'arched', 769: 'cut', 770: 'times', 771: 'beneath', 772: 'covered', 773: 'gathered', 774: 'stands', 775: 'rude', 776: 'bone', 777: 'abominable', 778: 'measure', 779: 'seamen', 780: 'skrimshander', 781: 'sought', 782: 'added', 783: 'forehead', 784: 'objections', 785: \"'d\", 786: 'used', 787: 'depend', 788: 'decent', 789: 'want', 790: 'ready', 791: 'working', 792: 'space', 793: 'lips', 794: 'fingers', 795: 'fare', 796: 'fellow', 797: 'nightmare', 798: 'nt', 799: 'complexioned', 800: 'eats', 801: \"'em\", 802: 'afore', 803: 'resolved', 804: 'noise', 805: 'offing', 806: 'shaggy', 807: 'woollen', 808: 'stiff', 809: 'labrador', 810: 'wake', 811: 'bad', 812: 'mounted', 813: 'generally', 814: 'shipmates', 815: 'become', 816: 'height', 817: 'seem', 818: 'plan', 819: 'private', 820: 'unknown', 821: 'apartment', 822: 'hammock', 823: 'linen', 824: 'home', 825: 'hole', 826: 'mattress', 827: 'planing', 828: 'knot', 829: 'near', 830: 'sake', 831: 'chair', 832: 'narrow', 833: 'leaving', 834: 'interval', 835: 'met', 836: 'inside', 837: 'violent', 838: 'ones', 839: 'comprehension', 840: 'bird', 841: 'airley', 842: 'airth', 843: 'engaged', 844: 'whittling', 845: 'guess', 846: 'done', 847: 'break', 848: 'broke', 849: 'understand', 850: 'certain', 851: 'demand', 852: 'selling', 853: 'sir', 854: 'string', 855: 'mystery', 856: 'showed', 857: 'dangerous', 858: 'flukes', 859: 'slept', 860: 'big', 861: 'sam', 862: 'lighted', 863: 'wo', 864: 'stairs', 865: 'placing', 866: 'double', 867: 'eyeing', 868: 'belonging', 869: 'papered', 870: 'parcel', 871: 'tried', 872: 'satisfactory', 873: 'concerning', 874: 'edges', 875: 'stuck', 876: 'gave', 877: 'neck', 878: 'sleeves', 879: 'undressed', 880: 'remembering', 881: 'jumped', 882: 'pantaloons', 883: 'blowing', 884: 'doze', 885: 'pretty', 886: 'heavy', 887: 'save', 888: 'infernal', 889: 'peddler', 890: 'yellow', 891: 'colour', 892: 'dreadfully', 893: 'sticking', 894: 'cheeks', 895: 'truth', 896: 'remembered', 897: 'whaleman', 898: 'tattooed', 899: 'concluded', 900: 'lying', 901: 'tanning', 902: 'hot', 903: 'produced', 904: 'beaver', 905: 'singing', 906: 'bolted', 907: 'arms', 908: 'running', 909: 'curious', 910: 'hunch', 911: 'congo', 912: 'ill', 913: 'takes', 914: 'biscuit', 915: 'top', 916: 'lamp', 917: 'succeeded', 918: 'polite', 919: 'guttural', 920: 'pagan', 921: 'spell', 922: 'tobacco', 923: 'grunt', 924: 'whatever', 925: 'ee', 926: 'horrid', 927: 'pipe', 928: 'smoking', 929: 'complied', 930: 'patchwork', 931: 'figure', 932: 'shade', 933: 'quilt', 934: 'hugging', 935: 'sensations', 936: 'explain', 937: 'remember', 938: 'circumstance', 939: 'reality', 940: 'stepmother', 941: 'sixteen', 942: 'abed', 943: 'troubled', 944: 'supernatural', 945: 'ages', 946: 'awful', 947: 'consciousness', 948: 'observing', 949: 'creature', 950: 'dress', 951: 'watching', 952: 'probably', 953: 'toilet', 954: 'wrapped', 955: 'precisely', 956: 'jacket', 957: 'call', 958: 'shore', 959: 'watery', 960: 'driving', 961: 'spleen', 962: 'regulating', 963: 'circulation', 964: 'growing', 965: 'grim', 966: 'drizzly', 967: 'november', 968: 'bringing', 969: 'rear', 970: 'funeral', 971: 'meet', 972: 'hypos', 973: 'upper', 974: 'moral', 975: 'principle', 976: 'prevent', 977: 'deliberately', 978: 'stepping', 979: 'methodically', 980: 'knocking', 981: 'hats', 982: 'substitute', 983: 'pistol', 984: 'ball', 985: 'philosophical', 986: 'flourish', 987: 'cato', 988: 'throws', 989: 'surprising', 990: 'cherish', 991: 'feelings', 992: 'insular', 993: 'manhattoes', 994: 'belted', 995: 'wharves', 996: 'isles', 997: 'coral', 998: 'reefs', 999: 'commerce', 1000: 'surrounds', 1001: 'surf', 1002: 'extreme', 1003: 'downtown', 1004: 'mole', 1005: 'waves', 1006: 'cooled', 1007: 'breezes', 1008: 'gazers', 1009: 'circumambulate', 1010: 'dreamy', 1011: 'corlears', 1012: 'hook', 1013: 'coenties', 1014: 'slip', 1015: 'whitehall', 1016: 'northward', 1017: 'see?--posted', 1018: 'sentinels', 1019: 'mortal', 1020: 'spiles', 1021: 'pier', 1022: 'china', 1023: 'rigging', 1024: 'striving', 1025: 'seaward', 1026: 'peep', 1027: 'landsmen', 1028: 'pent', 1029: 'lath', 1030: 'tied', 1031: 'counters', 1032: 'nailed', 1033: 'benches', 1034: 'clinched', 1035: 'desks', 1036: 'fields', 1037: 'pacing', 1038: 'seemingly', 1039: 'dive', 1040: 'extremest', 1041: 'limit', 1042: 'loitering', 1043: 'shady', 1044: 'lee', 1045: 'suffice', 1046: 'possibly', 1047: 'leagues', 1048: 'inlanders', 1049: 'lanes', 1050: 'alleys', 1051: 'avenues', 1052: 'east', 1053: 'west', 1054: 'unite', 1055: 'magnetic', 1056: 'virtue', 1057: 'needles', 1058: 'compasses', 1059: 'attract', 1060: 'thither', 1061: 'lakes', 1062: 'path', 1063: 'carries', 1064: 'dale', 1065: 'pool', 1066: 'absent', 1067: 'minded', 1068: 'deepest', 1069: 'infallibly', 1070: 'region', 1071: 'athirst', 1072: 'desert', 1073: 'experiment', 1074: 'caravan', 1075: 'happen', 1076: 'supplied', 1077: 'professor', 1078: 'knows', 1079: 'meditation', 1080: 'wedded', 1081: 'desires', 1082: 'paint', 1083: 'dreamiest', 1084: 'shadiest', 1085: 'quietest', 1086: 'enchanting', 1087: 'romantic', 1088: 'landscape', 1089: 'valley', 1090: 'saco', 1091: 'element', 1092: 'employs', 1093: 'trees', 1094: 'hollow', 1095: 'hermit', 1096: 'crucifix', 1097: 'sleeps', 1098: 'cattle', 1099: 'cottage', 1100: 'sleepy', 1101: 'woodlands', 1102: 'mazy', 1103: 'overlapping', 1104: 'spurs', 1105: 'mountains', 1106: 'bathed', 1107: 'lies', 1108: 'tranced', 1109: 'tree', 1110: 'shakes', 1111: 'vain', 1112: 'eye', 1113: 'visit', 1114: 'prairies', 1115: 'wade', 1116: 'knee', 1117: 'tiger', 1118: 'lilies', 1119: 'charm', 1120: 'wanting?--water', 1121: 'drop', 1122: 'niagara', 1123: 'cataract', 1124: 'sand', 1125: 'travel', 1126: 'poet', 1127: 'tennessee', 1128: 'receiving', 1129: 'handfuls', 1130: 'silver', 1131: 'deliberate', 1132: 'buy', 1133: 'needed', 1134: 'invest', 1135: 'pedestrian', 1136: 'trip', 1137: 'rockaway', 1138: 'beach', 1139: 'mystical', 1140: 'vibration', 1141: 'persians', 1142: 'greeks', 1143: 'separate', 1144: 'deity', 1145: 'jove', 1146: 'surely', 1147: 'deeper', 1148: 'narcissus', 1149: 'grasp', 1150: 'tormenting', 1151: 'mild', 1152: 'fountain', 1153: 'drowned', 1154: 'rivers', 1155: 'oceans', 1156: 'ungraspable', 1157: 'key', 1158: 'habit', 1159: 'hazy', 1160: 'conscious', 1161: 'lungs', 1162: 'rag', 1163: 'sick', 1164: 'quarrelsome', 1165: 'nights', 1166: 'enjoy', 1167: 'general', 1168: 'thing;--no', 1169: 'salt', 1170: 'abandon', 1171: 'distinction', 1172: 'offices', 1173: 'abominate', 1174: 'honourable', 1175: 'respectable', 1176: 'toils', 1177: 'trials', 1178: 'tribulations', 1179: 'barques', 1180: 'brigs', 1181: 'schooners', 1182: 'cook,--though', 1183: 'considerable', 1184: 'fancied', 1185: 'broiling', 1186: 'fowls;--though', 1187: 'judiciously', 1188: 'buttered', 1189: 'judgmatically', 1190: 'salted', 1191: 'peppered', 1192: 'reverentially', 1193: 'fowl', 1194: 'idolatrous', 1195: 'dotings', 1196: 'egyptians', 1197: 'ibis', 1198: 'roasted', 1199: 'river', 1200: 'mummies', 1201: 'creatures', 1202: 'bake', 1203: 'pyramids', 1204: 'simple', 1205: 'plumb', 1206: 'royal', 1207: 'grasshopper', 1208: 'unpleasant', 1209: 'touches', 1210: 'honour', 1211: 'established', 1212: 'family', 1213: 'van', 1214: 'rensselaers', 1215: 'randolphs', 1216: 'hardicanutes', 1217: 'pot', 1218: 'lording', 1219: 'tallest', 1220: 'awe', 1221: 'keen', 1222: 'assure', 1223: 'decoction', 1224: 'seneca', 1225: 'stoics', 1226: 'enable', 1227: 'wears', 1228: 'orders', 1229: 'broom', 1230: 'decks', 1231: 'indignity', 1232: 'amount', 1233: 'scales', 1234: 'testament', 1235: 'archangel', 1236: 'gabriel', 1237: 'promptly', 1238: 'obey', 1239: 'instance', 1240: 'slave', 1241: 'captains', 1242: 'punch', 1243: 'satisfaction', 1244: 'everybody', 1245: 'served', 1246: 'physical', 1247: 'universal', 1248: 'rub', 1249: 'shoulder', 1250: 'blades', 1251: 'trouble', 1252: 'whereas', 1253: 'contrary', 1254: 'paid', 1255: 'infliction', 1256: 'orchard', 1257: 'thieves', 1258: 'entailed', 1259: 'paid,--what', 1260: 'urbane', 1261: 'activity', 1262: 'receives', 1263: 'considering', 1264: 'earnestly', 1265: 'believe', 1266: 'root', 1267: 'ills', 1268: 'monied', 1269: 'ah', 1270: 'cheerfully', 1271: 'consign', 1272: 'perdition', 1273: 'finally', 1274: 'wholesome', 1275: 'exercise', 1276: 'pure', 1277: 'fore', 1278: 'castle', 1279: 'prevalent', 1280: 'astern', 1281: 'violate', 1282: 'pythagorean', 1283: 'maxim', 1284: 'gets', 1285: 'atmosphere', 1286: 'breathes', 1287: 'commonalty', 1288: 'suspect', 1289: 'wherefore', 1290: 'repeatedly', 1291: 'merchant', 1292: 'invisible', 1293: 'police', 1294: 'constant', 1295: 'surveillance', 1296: 'secretly', 1297: 'dogs', 1298: 'influences', 1299: 'programme', 1300: 'providence', 1301: 'drawn', 1302: 'brief', 1303: 'interlude', 1304: 'solo', 1305: 'extensive', 1306: 'performances', 1307: 'bill', 1308: 'contested', 1309: 'election', 1310: 'presidency', 1311: 'united', 1312: 'states', 1313: 'bloody', 1314: 'battle', 1315: 'affghanistan', 1316: 'managers', 1317: 'magnificent', 1318: 'tragedies', 1319: 'genteel', 1320: 'comedies', 1321: 'farces', 1322: 'recall', 1323: 'springs', 1324: 'cunningly', 1325: 'presented', 1326: 'disguises', 1327: 'induced', 1328: 'performing', 1329: 'cajoling', 1330: 'delusion', 1331: 'choice', 1332: 'resulting', 1333: 'unbiased', 1334: 'freewill', 1335: 'discriminating', 1336: 'judgment', 1337: 'overwhelming', 1338: 'monster', 1339: 'roused', 1340: 'bulk', 1341: 'undeliverable', 1342: 'perils', 1343: 'attending', 1344: 'marvels', 1345: 'patagonian', 1346: 'sights', 1347: 'helped', 1348: 'sway', 1349: 'wish', 1350: 'inducements', 1351: 'itch', 1352: 'remote', 1353: 'love', 1354: 'forbidden', 1355: 'barbarous', 1356: 'coasts', 1357: 'ignoring', 1358: 'quick', 1359: 'perceive', 1360: 'horror', 1361: 'social', 1362: 'friendly', 1363: 'terms', 1364: 'inmates', 1365: 'lodges', 1366: 'welcome', 1367: 'flood', 1368: 'gates', 1369: 'swung', 1370: 'conceits', 1371: 'swayed', 1372: 'inmost', 1373: 'endless', 1374: 'processions', 1375: 'mid', 1376: 'hooded', 1377: 'carpet', 1378: 'tucked', 1379: 'started', 1380: 'pacific', 1381: 'quitting', 1382: 'manhatto', 1383: 'duly', 1384: 'december', 1385: 'disappointed', 1386: 'learning', 1387: 'packet', 1388: 'already', 1389: 'sailed', 1390: 'monday', 1391: 'candidates', 1392: 'pains', 1393: 'penalties', 1394: 'related', 1395: 'doing', 1396: 'boisterous', 1397: 'everything', 1398: 'connected', 1399: 'famous', 1400: 'amazingly', 1401: 'gradually', 1402: 'monopolising', 1403: 'behind', 1404: 'tyre', 1405: 'carthage;--the', 1406: 'aboriginal', 1407: 'whalemen', 1408: 'sally', 1409: 'canoes', 1410: 'chase', 1411: 'adventurous', 1412: 'sloop', 1413: 'forth', 1414: 'laden', 1415: 'imported', 1416: 'cobblestones', 1417: 'throw', 1418: 'discover', 1419: 'risk', 1420: 'bowsprit', 1421: 'destined', 1422: 'port', 1423: 'concernment', 1424: 'eat', 1425: 'dubious', 1426: 'bitingly', 1427: 'cheerless', 1428: 'anxious', 1429: 'grapnels', 1430: 'sounded', 1431: 'brought', 1432: 'pieces', 1433: 'silver,--so', 1434: 'shouldering', 1435: 'comparing', 1436: 'gloom', 1437: 'wisdom', 1438: 'conclude', 1439: 'lodge', 1440: 'dear', 1441: 'inquire', 1442: 'price', 1443: 'halting', 1444: 'steps', 1445: 'paced', 1446: 'harpoons\"--but', 1447: 'fervent', 1448: 'rays', 1449: 'melted', 1450: 'everywhere', 1451: 'congealed', 1452: 'asphaltic', 1453: 'pavement,--rather', 1454: 'weary', 1455: 'struck', 1456: 'flinty', 1457: 'projections', 1458: 'remorseless', 1459: 'service', 1460: 'soles', 1461: 'miserable', 1462: 'plight', 1463: 'glare', 1464: 'patched', 1465: 'stopping', 1466: 'instinct', 1467: 'followed', 1468: 'cheapest', 1469: 'cheeriest', 1470: 'inns', 1471: 'blocks', 1472: 'tomb', 1473: 'deserted', 1474: 'smoky', 1475: 'proceeding', 1476: 'building', 1477: 'invitingly', 1478: 'careless', 1479: 'uses', 1480: 'stumble', 1481: 'ash', 1482: 'porch', 1483: 'particles', 1484: 'choked', 1485: 'destroyed', 1486: 'gomorrah', 1487: 'fish?\"--this', 1488: 'picked', 1489: 'hearing', 1490: 'pushed', 1491: 'interior', 1492: 'parliament', 1493: 'tophet', 1494: 'hundred', 1495: 'faces', 1496: 'rows', 1497: 'peer', 1498: 'angel', 1499: 'doom', 1500: 'beating', 1501: 'book', 1502: 'pulpit', 1503: 'church', 1504: 'preacher', 1505: 'text', 1506: 'weeping', 1507: 'wailing', 1508: 'gnashing', 1509: 'muttered', 1510: 'backing', 1511: 'wretched', 1512: 'entertainment', 1513: 'docks', 1514: 'forlorn', 1515: 'faintly', 1516: 'jet', 1517: 'misty', 1518: 'spray', 1519: 'words', 1520: 'underneath--\"the', 1521: 'inn:--peter', 1522: 'coffin?--spouter?--rather', 1523: 'ominous', 1524: 'common', 1525: 'emigrant', 1526: 'dilapidated', 1527: 'ruins', 1528: 'district', 1529: 'poverty', 1530: 'stricken', 1531: 'creak', 1532: 'cheap', 1533: 'lodgings', 1534: 'pea', 1535: 'coffee', 1536: 'palsied', 1537: 'bleak', 1538: 'paul', 1539: 'tossed', 1540: 'mighty', 1541: 'pleasant', 1542: 'zephyr', 1543: 'doors', 1544: 'hob', 1545: 'toasting', 1546: 'judging', 1547: 'writer', 1548: 'whose', 1549: 'works', 1550: 'possess', 1551: 'copy', 1552: 'extant--\"it', 1553: 'maketh', 1554: 'lookest', 1555: 'observest', 1556: 'sashless', 1557: 'sides', 1558: 'wight', 1559: 'glazier', 1560: 'letter', 1561: 'reasonest', 1562: 'pity', 1563: 'chinks', 1564: 'crannies', 1565: 'thrust', 1566: 'lint', 1567: 'improvements', 1568: 'universe', 1569: 'finished', 1570: 'copestone', 1571: 'chips', 1572: 'million', 1573: 'chattering', 1574: 'pillow', 1575: 'shaking', 1576: 'tatters', 1577: 'shiverings', 1578: 'plug', 1579: 'ears', 1580: 'rags', 1581: 'cob', 1582: 'silken', 1583: 'wrapper--(he', 1584: 'redder', 1585: 'frosty', 1586: 'orion', 1587: 'glitters', 1588: 'talk', 1589: 'oriental', 1590: 'climes', 1591: 'conservatories', 1592: 'privilege', 1593: 'coals', 1594: 'warm', 1595: 'sumatra', 1596: 'line', 1597: 'equator', 1598: 'yea', 1599: 'fiery', 1600: 'pit', 1601: 'wonderful', 1602: 'iceberg', 1603: 'moored', 1604: 'moluccas', 1605: 'lives', 1606: 'czar', 1607: 'palace', 1608: 'president', 1609: 'temperance', 1610: 'society', 1611: 'drinks', 1612: 'tepid', 1613: 'tears', 1614: 'orphans', 1615: 'blubbering', 1616: 'scrape', 1617: 'frosted', 1618: 'straggling', 1619: 'fashioned', 1620: 'wainscots', 1621: 'reminding', 1622: 'condemned', 1623: 'oilpainting', 1624: 'thoroughly', 1625: 'besmoked', 1626: 'defaced', 1627: 'unequal', 1628: 'crosslights', 1629: 'viewed', 1630: 'diligent', 1631: 'systematic', 1632: 'visits', 1633: 'careful', 1634: 'inquiry', 1635: 'neighbors', 1636: 'understanding', 1637: 'masses', 1638: 'shades', 1639: 'ambitious', 1640: 'england', 1641: 'hags', 1642: 'endeavored', 1643: 'delineate', 1644: 'chaos', 1645: 'bewitched', 1646: 'earnest', 1647: 'contemplation', 1648: 'oft', 1649: 'repeated', 1650: 'ponderings', 1651: 'unwarranted', 1652: 'puzzled', 1653: 'mass', 1654: 'hovering', 1655: 'perpendicular', 1656: 'lines', 1657: 'floating', 1658: 'yeast', 1659: 'boggy', 1660: 'soggy', 1661: 'squitchy', 1662: 'nervous', 1663: 'distracted', 1664: 'indefinite', 1665: 'attained', 1666: 'sublimity', 1667: 'fairly', 1668: 'froze', 1669: 'oath', 1670: 'anon', 1671: 'alas', 1672: 'deceptive', 1673: 'dart', 1674: 'through.--it', 1675: 'gale.--it', 1676: 'combat', 1677: 'primal', 1678: 'elements.--it', 1679: 'blasted', 1680: 'heath.--it', 1681: 'hyperborean', 1682: 'winter', 1683: 'scene.--it', 1684: 'icebound', 1685: 'fancies', 1686: 'yielded', 1687: 'midst', 1688: 'faint', 1689: 'resemblance', 1690: 'gigantic', 1691: 'final', 1692: 'theory', 1693: 'based', 1694: 'aggregated', 1695: 'opinions', 1696: 'aged', 1697: 'persons', 1698: 'conversed', 1699: 'subject', 1700: 'represents', 1701: 'horner', 1702: 'hurricane', 1703: 'foundered', 1704: 'weltering', 1705: 'dismantled', 1706: 'masts', 1707: 'alone', 1708: 'visible', 1709: 'exasperated', 1710: 'purposing', 1711: 'spring', 1712: 'enormous', 1713: 'impaling', 1714: 'heathenish', 1715: 'array', 1716: 'clubs', 1717: 'spears', 1718: 'thickly', 1719: 'glittering', 1720: 'resembling', 1721: 'ivory', 1722: 'saws', 1723: 'tufted', 1724: 'sickle', 1725: 'shaped', 1726: 'sweeping', 1727: 'segment', 1728: 'mown', 1729: 'grass', 1730: 'armed', 1731: 'mower', 1732: 'shuddered', 1733: 'gazed', 1734: 'harvesting', 1735: 'hacking', 1736: 'horrifying', 1737: 'implement', 1738: 'rusty', 1739: 'lances', 1740: 'storied', 1741: 'weapons', 1742: 'lance', 1743: 'wildly', 1744: 'elbowed', 1745: 'fifty', 1746: 'nathan', 1747: 'swain', 1748: 'fifteen', 1749: 'sunrise', 1750: 'sunset', 1751: 'corkscrew', 1752: 'javan', 1753: 'slain', 1754: 'blanco', 1755: 'tail', 1756: 'restless', 1757: 'needle', 1758: 'sojourning', 1759: 'travelled', 1760: 'forty', 1761: 'imbedded', 1762: 'hump', 1763: 'crossing', 1764: 'dusky', 1765: 'yon', 1766: 'central', 1767: 'fireplaces', 1768: 'duskier', 1769: 'ponderous', 1770: 'beams', 1771: 'above', 1772: 'planks', 1773: 'trod', 1774: 'cockpits', 1775: 'anchored', 1776: 'ark', 1777: 'rocked', 1778: 'furiously', 1779: 'cracked', 1780: 'cases', 1781: 'filled', 1782: 'dusty', 1783: 'rarities', 1784: 'remotest', 1785: 'nooks', 1786: 'projecting', 1787: 'angle', 1788: 'den', 1789: 'attempt', 1790: 'jaw', 1791: 'coach', 1792: 'shelves', 1793: 'ranged', 1794: 'decanters', 1795: 'bottles', 1796: 'flasks', 1797: 'jaws', 1798: 'swift', 1799: 'destruction', 1800: 'cursed', 1801: 'bustles', 1802: 'withered', 1803: 'dearly', 1804: 'sells', 1805: 'deliriums', 1806: 'tumblers', 1807: 'pours', 1808: 'poison', 1809: 'cylinders', 1810: 'villanous', 1811: 'goggling', 1812: 'deceitfully', 1813: 'tapered', 1814: 'downwards', 1815: 'cheating', 1816: 'bottom', 1817: 'parallel', 1818: 'meridians', 1819: 'rudely', 1820: 'pecked', 1821: 'surround', 1822: 'footpads', 1823: 'goblets', 1824: 'fill', 1825: 'mark', 1826: 'charge', 1827: 'gulp', 1828: 'shilling', 1829: 'number', 1830: 'examining', 1831: 'divers', 1832: 'specimens', 1833: 'desired', 1834: 'accommodated', 1835: 'received', 1836: 'unoccupied', 1837: 'avast', 1838: 'tapping', 1839: 'haint', 1840: 'sharing', 1841: \"s'pose\", 1842: 'whalin', 1843: 'liked', 1844: 'decidedly', 1845: 'objectionable', 1846: 'wander', 1847: 'seat', 1848: 'supper?--you', 1849: \"supper'll\", 1850: 'directly', 1851: 'carved', 1852: 'end', 1853: 'ruminating', 1854: 'adorning', 1855: 'jack', 1856: 'knife', 1857: 'stooping', 1858: 'diligently', 1859: 'headway', 1860: 'five', 1861: 'summoned', 1862: 'meal', 1863: 'adjoining', 1864: 'iceland', 1865: 'afford', 1866: 'tallow', 1867: 'candles', 1868: 'winding', 1869: 'sheet', 1870: 'fain', 1871: 'button', 1872: 'jackets', 1873: 'cups', 1874: 'scalding', 1875: 'tea', 1876: 'substantial', 1877: 'meat', 1878: 'potatoes', 1879: 'addressed', 1880: 'direful', 1881: 'sartainty', 1882: 'whispered', 1883: 'oh', 1884: 'diabolically', 1885: 'funny', 1886: 'steaks', 1887: 'likes', 1888: 'rare', 1889: 'suspicious', 1890: 'rate', 1891: 'undress', 1892: 'company', 1893: 'evening', 1894: 'looker', 1895: 'rioting', 1896: 'starting', 1897: 'cried', 1898: 'grampus', 1899: 'crew', 1900: 'seed', 1901: 'reported', 1902: 'hurrah', 1903: 'latest', 1904: 'news', 1905: 'feegees', 1906: 'tramping', 1907: 'mariners', 1908: 'enveloped', 1909: 'coats', 1910: 'muffled', 1911: 'comforters', 1912: 'bedarned', 1913: 'ragged', 1914: 'beards', 1915: 'icicles', 1916: 'eruption', 1917: 'bears', 1918: 'boat', 1919: 'officiating', 1920: 'poured', 1921: 'brimmers', 1922: 'complained', 1923: 'pitch', 1924: 'potion', 1925: 'gin', 1926: 'molasses', 1927: 'swore', 1928: 'sovereign', 1929: 'cure', 1930: 'colds', 1931: 'catarrhs', 1932: 'caught', 1933: 'coast', 1934: 'weather', 1935: 'liquor', 1936: 'arrantest', 1937: 'topers', 1938: 'newly', 1939: 'capering', 1940: 'obstreperously', 1941: 'observed', 1942: 'aloof', 1943: 'desirous', 1944: 'spoil', 1945: 'hilarity', 1946: 'refrained', 1947: 'interested', 1948: 'ordained', 1949: 'shipmate', 1950: 'partner', 1951: 'narrative', 1952: 'concerned', 1953: 'venture', 1954: 'description', 1955: 'six', 1956: 'shoulders', 1957: 'coffer', 1958: 'seldom', 1959: 'brawn', 1960: 'deeply', 1961: 'dazzling', 1962: 'contrast', 1963: 'reminiscences', 1964: 'joy', 1965: 'announced', 1966: 'southerner', 1967: 'stature', 1968: 'mountaineers', 1969: 'alleghanian', 1970: 'ridge', 1971: 'virginia', 1972: 'revelry', 1973: 'companions', 1974: 'slipped', 1975: 'unobserved', 1976: 'comrade', 1977: 'minutes', 1978: 'missed', 1979: 'seems', 1980: 'favourite', 1981: 'raised', 1982: 'cry', 1983: 'darted', 1984: 'pursuit', 1985: 'nine', 1986: 'seeming', 1987: 'supernaturally', 1988: 'orgies', 1989: 'congratulate', 1990: 'entrance', 1991: 'prefers', 1992: 'comes', 1993: 'indefinitely', 1994: 'multiply', 1995: 'anybody', 1996: 'bachelor', 1997: 'kings', 1998: 'ashore', 1999: 'cover', 2000: 'pondered', 2001: 'abominated', 2002: 'fair', 2003: 'presume', 2004: 'case', 2005: 'tidiest', 2006: 'certainly', 2007: 'finest', 2008: 'twitch', 2009: 'ought', 2010: 'bedwards', 2011: 'tumble', 2012: 'vile', 2013: 'changed', 2014: 'harpooneer.--i', 2015: 'sha', 2016: \"'m\", 2017: 'sorry', 2018: 'spare', 2019: 'tablecloth', 2020: 'plaguy', 2021: 'rough', 2022: 'here\"--feeling', 2023: 'notches', 2024: 'carpenter', 2025: 'snug', 2026: 'procured', 2027: 'silk', 2028: 'handkerchief', 2029: 'dusting', 2030: 'vigorously', 2031: 'ape', 2032: 'flew', 2033: 'bump', 2034: 'indestructible', 2035: 'spraining', 2036: 'wrist', 2037: 'quit', 2038: 'soft', 2039: 'suit', 2040: 'eider', 2041: 'plank', 2042: 'gathering', 2043: 'stove', 2044: 'mended', 2045: 'higher', 2046: 'planed', 2047: 'yoking', 2048: 'clear', 2049: 'draught', 2050: 'sill', 2051: 'current', 2052: 'rickety', 2053: 'whirlwinds', 2054: 'immediate', 2055: 'vicinity', 2056: 'fetch', 2057: 'steal', 2058: 'march', 2059: 'bolt', 2060: 'wakened', 2061: 'knockings', 2062: 'thoughts', 2063: 'dismissed', 2064: 'popped', 2065: 'knock', 2066: 'chance', 2067: 'spending', 2068: 'sufferable', 2069: 'person', 2070: 'cherishing', 2071: 'unwarrantable', 2072: 'prejudices', 2073: 'awhile', 2074: 'dropping', 2075: 'bedfellows', 2076: 'boarders', 2077: 'twos', 2078: 'threes', 2079: 'twelve', 2080: 'chuckled', 2081: 'lean', 2082: 'chuckle', 2083: 'mightily', 2084: 'tickled', 2085: 'answered', 2086: 'early', 2087: 'rise', 2088: 'catches', 2089: 'worm', 2090: 'keeps', 2091: 'head?--what', 2092: 'bamboozingly', 2093: 'towering', 2094: 'rage', 2095: 'pretend', 2096: 'actually', 2097: 'blessed', 2098: 'market', 2099: 'overstocked', 2100: 'calmly', 2101: 'spinning', 2102: 'yarn', 2103: \"i'm\", 2104: 'stick', 2105: 'toothpick', 2106: 'rayther', 2107: 'hears', 2108: 'slanderin', 2109: 'passion', 2110: 'farrago', 2111: \"a'ready\", 2112: 'i--\"broke', 2113: 'sartain', 2114: 'cool', 2115: 'mt.', 2116: 'hecla', 2117: 'storm--\"landlord', 2118: 'delay', 2119: 'belongs', 2120: 'persist', 2121: 'mystifying', 2122: 'exasperating', 2123: 'stories', 2124: 'tending', 2125: 'beget', 2126: 'intimate', 2127: 'confidential', 2128: 'highest', 2129: 'shall', 2130: 'respects', 2131: 'safe', 2132: 'unsay', 2133: 'evidence', 2134: 'stark', 2135: 'mad', 2136: 'madman', 2137: 'induce', 2138: 'knowingly', 2139: 'thereby', 2140: 'render', 2141: 'liable', 2142: 'criminal', 2143: 'prosecution', 2144: 'fetching', 2145: 'breath', 2146: 'purty', 2147: 'sarmon', 2148: 'rips', 2149: 'tellin', 2150: 'bought', 2151: 'lot', 2152: 'balmed', 2153: 'curios', 2154: 'sold', 2155: 'cause', 2156: 'morrow', 2157: 'sellin', 2158: 'folks', 2159: 'churches', 2160: 'wanted', 2161: 'stopped', 2162: 'strung', 2163: 'inions', 2164: 'cleared', 2165: 'otherwise', 2166: 'fooling', 2167: 'stayed', 2168: 'idolators', 2169: 'pays', 2170: \"reg'lar\", 2171: 'rejoinder', 2172: 'dreadful', 2173: 'turning', 2174: 'nice', 2175: 'spliced', 2176: 'kick', 2177: 'almighty', 2178: 'johnny', 2179: 'dreaming', 2180: 'sprawling', 2181: 'pitched', 2182: 'arter', 2183: 'glim', 2184: 'jiffy', 2185: 'offering', 2186: 'irresolute', 2187: 'clock', 2188: 'exclaimed', 2189: 'vum', 2190: 'anchor', 2191: 'somewhere', 2192: \"won't\", 2193: 'considered', 2194: 'ushered', 2195: 'clam', 2196: 'furnished', 2197: 'prodigious', 2198: 'harpooneers', 2199: 'abreast', 2200: 'duty', 2201: 'comfortable', 2202: 'disappeared', 2203: 'folding', 2204: 'stooped', 2205: 'elegant', 2206: 'scrutiny', 2207: 'tolerably', 2208: 'glanced', 2209: 'bedstead', 2210: 'furniture', 2211: 'walls', 2212: 'fireboard', 2213: 'striking', 2214: 'properly', 2215: 'lashed', 2216: 'also', 2217: 'seaman', 2218: 'containing', 2219: 'wardrobe', 2220: 'doubt', 2221: 'lieu', 2222: 'likewise', 2223: 'outlandish', 2224: 'hooks', 2225: 'close', 2226: 'ornamented', 2227: 'tags', 2228: 'stained', 2229: 'porcupine', 2230: 'quills', 2231: 'moccasin', 2232: 'slit', 2233: 'ponchos', 2234: 'parade', 2235: 'guise', 2236: 'hamper', 2237: 'uncommonly', 2238: 'wearing', 2239: 'rainy', 2240: 'tore', 2241: 'hurry', 2242: 'kink', 2243: 'beginning', 2244: 'ado', 2245: 'tumbled', 2246: 'commended', 2247: 'cobs', 2248: 'crockery', 2249: 'slid', 2250: 'nod', 2251: 'footfall', 2252: 'glimmer', 2253: 'lord', 2254: 'perfectly', 2255: 'word', 2256: 'spoken', 2257: 'identical', 2258: 'knotted', 2259: 'cords', 2260: 'spoke', 2261: 'eagerness', 2262: 'averted', 2263: 'employed', 2264: 'unlacing', 2265: 'accomplished', 2266: 'blackish', 2267: 'terrible', 2268: 'fight', 2269: 'surgeon', 2270: 'chanced', 2271: 'plainly', 2272: 'plasters', 2273: 'stains', 2274: 'inkling', 2275: 'cannibals', 2276: 'course', 2277: 'voyages', 2278: 'adventure', 2279: 'honest', 2280: 'unearthly', 2281: 'complexion', 2282: 'independent', 2283: 'tattooing', 2284: 'tropical', 2285: 'extraordinary', 2286: 'effects', 2287: 'ideas', 2288: 'passing', 2289: 'lightning', 2290: 'noticed', 2291: 'difficulty', 2292: 'fumbling', 2293: 'pulled', 2294: 'seal', 2295: 'wallet', 2296: 'ghastly', 2297: 'crammed', 2298: 'fresh', 2299: 'surprise', 2300: 'least', 2301: 'scalp', 2302: 'twisted', 2303: 'bald', 2304: 'mildewed', 2305: 'skull', 2306: 'quicker', 2307: 'dinner', 2308: 'slipping', 2309: 'coward', 2310: 'purple', 2311: 'rascal', 2312: 'ignorance', 2313: 'parent', 2314: 'nonplussed', 2315: 'game', 2316: 'address', 2317: 'inexplicable', 2318: 'continued', 2319: 'undressing', 2320: 'live', 2321: 'checkered', 2322: 'thirty', 2323: 'war', 2324: 'escaped', 2325: 'marked', 2326: 'frogs', 2327: 'trunks', 2328: 'palms', 2329: 'shipped', 2330: 'aboard', 2331: 'quaked', 2332: 'brothers', 2333: 'shuddering', 2334: 'fascinated', 2335: 'attention', 2336: 'convinced', 2337: 'heathen', 2338: 'wrapall', 2339: 'dreadnaught', 2340: 'previously', 2341: 'fumbled', 2342: 'pockets', 2343: 'embalmed', 2344: 'manikin', 2345: 'real', 2346: 'preserved', 2347: 'glistened', 2348: 'polished', 2349: 'ebony', 2350: 'empty', 2351: 'removing', 2352: 'sets', 2353: 'backed', 2354: 'tenpin', 2355: 'andirons', 2356: 'jambs', 2357: 'bricks', 2358: 'sooty', 2359: 'appropriate', 2360: 'shrine', 2361: 'chapel', 2362: 'screwed', 2363: 'hidden', 2364: 'ease', 2365: 'meantime', 2366: 'follow', 2367: 'handful', 2368: 'places', 2369: 'carefully', 2370: 'laying', 2371: 'applying', 2372: 'flame', 2373: 'kindled', 2374: 'sacrificial', 2375: 'blaze', 2376: 'hasty', 2377: 'snatches', 2378: 'hastier', 2379: 'withdrawals', 2380: 'whereby', 2381: 'scorching', 2382: 'badly', 2383: 'drawing', 2384: 'heat', 2385: 'dry', 2386: 'moved', 2387: 'antics', 2388: 'accompanied', 2389: 'noises', 2390: 'devotee', 2391: 'praying', 2392: 'sing', 2393: 'song', 2394: 'psalmody', 2395: 'during', 2396: 'twitched', 2397: 'extinguishing', 2398: 'unceremoniously', 2399: 'bagged', 2400: 'carelessly', 2401: 'sportsman', 2402: 'bagging', 2403: 'woodcock', 2404: 'proceedings', 2405: 'increased', 2406: 'uncomfortableness', 2407: 'exhibiting', 2408: 'symptoms', 2409: 'concluding', 2410: 'operations', 2411: 'jumping', 2412: 'spent', 2413: 'deliberating', 2414: 'fatal', 2415: 'examined', 2416: 'instant', 2417: 'puffed', 2418: 'clouds', 2419: 'extinguished', 2420: 'sprang', 2421: 'sang', 2422: 'giving', 2423: 'sudden', 2424: 'astonishment', 2425: 'stammering', 2426: 'conjured', 2427: 'whoever', 2428: 'responses', 2429: 'satisfied', 2430: 'comprehended', 2431: 'debel', 2432: 'you?\"--he', 2433: 'said--\"you', 2434: 'e.', 2435: 'flourishing', 2436: 'god', 2437: 'angels', 2438: 'growled', 2439: 'flourishings', 2440: 'scattered', 2441: 'thank', 2442: 'leaping', 2443: 'ran', 2444: 'harm', 2445: \"know'd\", 2446: \"it;--didn't\", 2447: 'peddlin', 2448: 'town?--but', 2449: 'sleepe', 2450: 'plenty\"--grunted', 2451: 'puffing', 2452: 'gettee', 2453: 'motioning', 2454: 'clothes', 2455: 'civil', 2456: 'charitable', 2457: 'tattooings', 2458: 'comely', 2459: 'fuss', 2460: 'drunken', 2461: 'stash', 2462: 'insured', 2463: 'politely', 2464: 'motioned', 2465: 'rolling', 2466: 'touch', 2467: 'leg', 2468: 'daylight', 2469: 'loving', 2470: 'affectionate', 2471: 'wife', 2472: 'odd', 2473: 'parti', 2474: 'coloured', 2475: 'triangles', 2476: 'interminable', 2477: 'cretan', 2478: 'labyrinth', 2479: 'precise', 2480: 'owing', 2481: 'keeping', 2482: 'unmethodically', 2483: 'irregularly', 2484: 'strip', 2485: 'awoke', 2486: 'hardly', 2487: 'blended', 2488: 'hues', 2489: 'weight', 2490: 'pressure', 2491: 'child', 2492: 'befell', 2493: 'dream', 2494: 'entirely', 2495: 'cutting', 2496: 'caper', 2497: 'crawl', 2498: 'whipping', 2499: 'sending', 2500: 'supperless,--my', 2501: 'mother', 2502: 'dragged', 2503: '21st', 2504: 'longest', 2505: 'year', 2506: 'hemisphere', 2507: 'third', 2508: 'sigh', 2509: 'sheets', 2510: 'dismally', 2511: 'calculating', 2512: 'entire', 2513: 'elapse', 2514: 'hope', 2515: 'resurrection', 2516: 'ached', 2517: 'shining', 2518: 'rattling', 2519: 'coaches', 2520: 'sound', 2521: 'gay', 2522: 'voices', 2523: 'dressed', 2524: 'softly', 2525: 'stockinged', 2526: 'threw', 2527: 'beseeching', 2528: 'favour', 2529: 'slippering', 2530: 'misbehaviour', 2531: 'condemning', 2532: 'unendurable', 2533: 'she', 2534: 'conscientious', 2535: 'stepmothers', 2536: 'several', 2537: 'awake', 2538: 'greatest', 2539: 'subsequent', 2540: 'misfortunes', 2541: 'fallen', 2542: 'steeped', 2543: 'dreams', 2544: 'lit', 2545: 'outer', 2546: 'instantly', 2547: 'shock', 2548: 'frame', 2549: 'form', 2550: 'belonged', 2551: 'closely', 2552: 'piled', 2553: 'fears', 2554: 'daring', 2555: 'drag', 2556: 'stir', 2557: 'inch', 2558: 'glided', 2559: 'shudderingly', 2560: 'weeks', 2561: 'months', 2562: 'lost', 2563: 'confounding', 2564: 'attempts', 2565: 'often', 2566: 'puzzle', 2567: 'strangeness', 2568: 'experienced', 2569: 'past', 2570: 'events', 2571: 'soberly', 2572: 'recurred', 2573: 'alive', 2574: 'comical', 2575: 'predicament', 2576: 'move', 2577: 'unlock', 2578: 'bridegroom', 2579: 'clasp', 2580: 'hugged', 2581: 'tightly', 2582: 'naught', 2583: 'twain', 2584: 'strove', 2585: 'rouse', 2586: 'him--\"queequeg!\"--but', 2587: 'snore', 2588: 'collar', 2589: 'slight', 2590: 'scratch', 2591: 'aside', 2592: 'hatchet', 2593: 'faced', 2594: 'pickle', 2595: 'queequeg!--in', 2596: 'goodness', 2597: 'wriggling', 2598: 'incessant', 2599: 'expostulations', 2600: 'unbecomingness', 2601: 'male', 2602: 'matrimonial', 2603: 'style', 2604: 'extracting', 2605: 'drew', 2606: 'shook', 2607: 'newfoundland', 2608: 'dog', 2609: 'pike', 2610: 'staff', 2611: 'rubbing', 2612: 'dawning', 2613: 'serious', 2614: 'misgivings', 2615: 'bent', 2616: 'narrowly', 2617: 'touching', 2618: 'character', 2619: 'reconciled', 2620: 'signs', 2621: 'leave', 2622: 'overture', 2623: 'savages', 2624: 'innate', 2625: 'delicacy', 2626: 'essentially', 2627: 'compliment', 2628: 'treated', 2629: 'civility', 2630: 'consideration', 2631: 'guilty', 2632: 'rudeness', 2633: 'staring', 2634: 'toilette', 2635: 'motions', 2636: 'breeding', 2637: 'ways', 2638: 'worth', 2639: 'unusual', 2640: 'regarding', 2641: 'dressing', 2642: 'donning', 2643: 'minus', 2644: 'trowsers', 2645: 'hunted', 2646: 'movement', 2647: 'crush', 2648: 'sundry', 2649: 'gaspings', 2650: 'strainings', 2651: 'work', 2652: 'booting', 2653: 'law', 2654: 'propriety', 2655: 'required', 2656: 'neither', 2657: 'caterpillar', 2658: 'butterfly', 2659: 'show', 2660: 'outlandishness', 2661: 'strangest', 2662: 'manners', 2663: 'education', 2664: 'completed', 2665: 'undergraduate', 2666: 'dreamt', 2667: 'emerged', 2668: 'dented', 2669: 'crushed', 2670: 'limping', 2671: 'accustomed', 2672: 'pair', 2673: 'cowhide', 2674: 'pinched', 2675: 'curtains', 2676: 'commanded', 2677: 'indecorous', 2678: 'staving', 2679: 'begged', 2680: 'accelerate', 2681: 'proceeded', 2682: 'amazement', 2683: 'contented', 2684: 'restricting', 2685: 'ablutions', 2686: 'donned', 2687: 'waistcoat', 2688: 'piece', 2689: 'soap', 2690: 'dipped', 2691: 'lathering', 2692: 'razor', 2693: 'lo', 2694: 'behold', 2695: 'slips', 2696: 'stock', 2697: 'unsheathes', 2698: 'whets', 2699: 'boot', 2700: 'striding', 2701: 'mirror', 2702: 'begins', 2703: 'vigorous', 2704: 'scraping', 2705: 'harpooning', 2706: 'using', 2707: 'rogers', 2708: 'cutlery', 2709: 'vengeance', 2710: 'operation', 2711: 'steel', 2712: 'exceedingly', 2713: 'achieved', 2714: 'interest', 2715: 'proudly', 2716: 'marched', 2717: 'pilot', 2718: 'sporting', 2719: 'marshal', 2720: 'baton'}\n",
      "[957, 15, 264, 3, 52, 262, 409, 88, 220, 130, 112, 955, 261, 51, 44, 39, 315, 8, 24, 547, 4, 151, 260, 7, 2714, 15]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word) # index of every word\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0197a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4909588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5acc9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29024782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1] # first 25 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3357520",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1] # last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80b6f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = keras.utils.np_utils.to_categorical(y, num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bab4cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1] # 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38b63fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f56144be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, seq_len):\n",
    "    model = Sequential()\n",
    "    # Embedding turns positive integers into dense vectors of fixed size (must be first layer)\n",
    "    model.add(Embedding(vocab_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(seq_len * 6, return_sequences=True)) # good to go with a multiple of seq_len\n",
    "    model.add(LSTM(seq_len * 6))\n",
    "    model.add(Dense(seq_len * 6, activation = 'relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fe1fd332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 25, 25)            68025     \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 150)               180600    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2721)              410871    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787,746\n",
      "Trainable params: 787,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size+1, seq_len) # add 1 as 0 is reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f32204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50cd8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "92/92 - 10s - loss: 6.7750 - accuracy: 0.0465 - 10s/epoch - 112ms/step\n",
      "Epoch 2/300\n",
      "92/92 - 7s - loss: 6.3187 - accuracy: 0.0511 - 7s/epoch - 76ms/step\n",
      "Epoch 3/300\n",
      "92/92 - 7s - loss: 6.2712 - accuracy: 0.0511 - 7s/epoch - 75ms/step\n",
      "Epoch 4/300\n",
      "92/92 - 7s - loss: 6.1375 - accuracy: 0.0511 - 7s/epoch - 75ms/step\n",
      "Epoch 5/300\n",
      "92/92 - 7s - loss: 6.0497 - accuracy: 0.0530 - 7s/epoch - 75ms/step\n",
      "Epoch 6/300\n",
      "92/92 - 7s - loss: 5.9249 - accuracy: 0.0615 - 7s/epoch - 76ms/step\n",
      "Epoch 7/300\n",
      "92/92 - 7s - loss: 5.7896 - accuracy: 0.0657 - 7s/epoch - 75ms/step\n",
      "Epoch 8/300\n",
      "92/92 - 7s - loss: 5.6755 - accuracy: 0.0680 - 7s/epoch - 75ms/step\n",
      "Epoch 9/300\n",
      "92/92 - 7s - loss: 5.5986 - accuracy: 0.0705 - 7s/epoch - 75ms/step\n",
      "Epoch 10/300\n",
      "92/92 - 7s - loss: 5.5317 - accuracy: 0.0743 - 7s/epoch - 75ms/step\n",
      "Epoch 11/300\n",
      "92/92 - 7s - loss: 5.4782 - accuracy: 0.0757 - 7s/epoch - 75ms/step\n",
      "Epoch 12/300\n",
      "92/92 - 7s - loss: 5.4235 - accuracy: 0.0783 - 7s/epoch - 76ms/step\n",
      "Epoch 13/300\n",
      "92/92 - 7s - loss: 5.3742 - accuracy: 0.0804 - 7s/epoch - 80ms/step\n",
      "Epoch 14/300\n",
      "92/92 - 8s - loss: 5.3272 - accuracy: 0.0815 - 8s/epoch - 86ms/step\n",
      "Epoch 15/300\n",
      "92/92 - 7s - loss: 5.2819 - accuracy: 0.0831 - 7s/epoch - 80ms/step\n",
      "Epoch 16/300\n",
      "92/92 - 7s - loss: 5.2366 - accuracy: 0.0849 - 7s/epoch - 76ms/step\n",
      "Epoch 17/300\n",
      "92/92 - 7s - loss: 5.1930 - accuracy: 0.0890 - 7s/epoch - 76ms/step\n",
      "Epoch 18/300\n",
      "92/92 - 7s - loss: 5.1535 - accuracy: 0.0859 - 7s/epoch - 76ms/step\n",
      "Epoch 19/300\n",
      "92/92 - 7s - loss: 5.1092 - accuracy: 0.0908 - 7s/epoch - 76ms/step\n",
      "Epoch 20/300\n",
      "92/92 - 7s - loss: 5.0632 - accuracy: 0.0903 - 7s/epoch - 76ms/step\n",
      "Epoch 21/300\n",
      "92/92 - 7s - loss: 5.0177 - accuracy: 0.0965 - 7s/epoch - 76ms/step\n",
      "Epoch 22/300\n",
      "92/92 - 7s - loss: 4.9671 - accuracy: 0.0938 - 7s/epoch - 75ms/step\n",
      "Epoch 23/300\n",
      "92/92 - 7s - loss: 4.9237 - accuracy: 0.0954 - 7s/epoch - 75ms/step\n",
      "Epoch 24/300\n",
      "92/92 - 7s - loss: 4.8711 - accuracy: 0.0969 - 7s/epoch - 75ms/step\n",
      "Epoch 25/300\n",
      "92/92 - 7s - loss: 4.8230 - accuracy: 0.0969 - 7s/epoch - 76ms/step\n",
      "Epoch 26/300\n",
      "92/92 - 7s - loss: 4.7790 - accuracy: 0.0992 - 7s/epoch - 76ms/step\n",
      "Epoch 27/300\n",
      "92/92 - 7s - loss: 4.7362 - accuracy: 0.0998 - 7s/epoch - 77ms/step\n",
      "Epoch 28/300\n",
      "92/92 - 7s - loss: 4.6930 - accuracy: 0.0973 - 7s/epoch - 76ms/step\n",
      "Epoch 29/300\n",
      "92/92 - 7s - loss: 4.6488 - accuracy: 0.1006 - 7s/epoch - 77ms/step\n",
      "Epoch 30/300\n",
      "92/92 - 7s - loss: 4.6047 - accuracy: 0.1017 - 7s/epoch - 76ms/step\n",
      "Epoch 31/300\n",
      "92/92 - 7s - loss: 4.5719 - accuracy: 0.1006 - 7s/epoch - 75ms/step\n",
      "Epoch 32/300\n",
      "92/92 - 7s - loss: 4.5309 - accuracy: 0.1054 - 7s/epoch - 76ms/step\n",
      "Epoch 33/300\n",
      "92/92 - 7s - loss: 4.4893 - accuracy: 0.1042 - 7s/epoch - 76ms/step\n",
      "Epoch 34/300\n",
      "92/92 - 7s - loss: 4.4550 - accuracy: 0.1036 - 7s/epoch - 76ms/step\n",
      "Epoch 35/300\n",
      "92/92 - 7s - loss: 4.4144 - accuracy: 0.1078 - 7s/epoch - 76ms/step\n",
      "Epoch 36/300\n",
      "92/92 - 7s - loss: 4.3797 - accuracy: 0.1073 - 7s/epoch - 77ms/step\n",
      "Epoch 37/300\n",
      "92/92 - 7s - loss: 4.3389 - accuracy: 0.1118 - 7s/epoch - 76ms/step\n",
      "Epoch 38/300\n",
      "92/92 - 7s - loss: 4.3027 - accuracy: 0.1114 - 7s/epoch - 76ms/step\n",
      "Epoch 39/300\n",
      "92/92 - 7s - loss: 4.2663 - accuracy: 0.1163 - 7s/epoch - 76ms/step\n",
      "Epoch 40/300\n",
      "92/92 - 7s - loss: 4.2305 - accuracy: 0.1156 - 7s/epoch - 78ms/step\n",
      "Epoch 41/300\n",
      "92/92 - 7s - loss: 4.1932 - accuracy: 0.1185 - 7s/epoch - 80ms/step\n",
      "Epoch 42/300\n",
      "92/92 - 7s - loss: 4.1542 - accuracy: 0.1231 - 7s/epoch - 77ms/step\n",
      "Epoch 43/300\n",
      "92/92 - 7s - loss: 4.1212 - accuracy: 0.1218 - 7s/epoch - 76ms/step\n",
      "Epoch 44/300\n",
      "92/92 - 7s - loss: 4.0867 - accuracy: 0.1265 - 7s/epoch - 76ms/step\n",
      "Epoch 45/300\n",
      "92/92 - 7s - loss: 4.0536 - accuracy: 0.1273 - 7s/epoch - 76ms/step\n",
      "Epoch 46/300\n",
      "92/92 - 7s - loss: 4.0154 - accuracy: 0.1317 - 7s/epoch - 76ms/step\n",
      "Epoch 47/300\n",
      "92/92 - 7s - loss: 3.9793 - accuracy: 0.1338 - 7s/epoch - 76ms/step\n",
      "Epoch 48/300\n",
      "92/92 - 7s - loss: 3.9482 - accuracy: 0.1369 - 7s/epoch - 77ms/step\n",
      "Epoch 49/300\n",
      "92/92 - 7s - loss: 3.9135 - accuracy: 0.1400 - 7s/epoch - 78ms/step\n",
      "Epoch 50/300\n",
      "92/92 - 7s - loss: 3.8767 - accuracy: 0.1424 - 7s/epoch - 75ms/step\n",
      "Epoch 51/300\n",
      "92/92 - 7s - loss: 3.8439 - accuracy: 0.1431 - 7s/epoch - 78ms/step\n",
      "Epoch 52/300\n",
      "92/92 - 7s - loss: 3.8057 - accuracy: 0.1520 - 7s/epoch - 78ms/step\n",
      "Epoch 53/300\n",
      "92/92 - 7s - loss: 3.7740 - accuracy: 0.1524 - 7s/epoch - 76ms/step\n",
      "Epoch 54/300\n",
      "92/92 - 7s - loss: 3.7387 - accuracy: 0.1623 - 7s/epoch - 75ms/step\n",
      "Epoch 55/300\n",
      "92/92 - 7s - loss: 3.7137 - accuracy: 0.1632 - 7s/epoch - 76ms/step\n",
      "Epoch 56/300\n",
      "92/92 - 7s - loss: 3.6734 - accuracy: 0.1661 - 7s/epoch - 77ms/step\n",
      "Epoch 57/300\n",
      "92/92 - 7s - loss: 3.6354 - accuracy: 0.1719 - 7s/epoch - 76ms/step\n",
      "Epoch 58/300\n",
      "92/92 - 7s - loss: 3.6050 - accuracy: 0.1779 - 7s/epoch - 75ms/step\n",
      "Epoch 59/300\n",
      "92/92 - 7s - loss: 3.5752 - accuracy: 0.1792 - 7s/epoch - 75ms/step\n",
      "Epoch 60/300\n",
      "92/92 - 7s - loss: 3.5512 - accuracy: 0.1762 - 7s/epoch - 75ms/step\n",
      "Epoch 61/300\n",
      "92/92 - 7s - loss: 3.5082 - accuracy: 0.1866 - 7s/epoch - 75ms/step\n",
      "Epoch 62/300\n",
      "92/92 - 7s - loss: 3.4784 - accuracy: 0.1923 - 7s/epoch - 75ms/step\n",
      "Epoch 63/300\n",
      "92/92 - 7s - loss: 3.4424 - accuracy: 0.1959 - 7s/epoch - 75ms/step\n",
      "Epoch 64/300\n",
      "92/92 - 7s - loss: 3.4101 - accuracy: 0.2016 - 7s/epoch - 77ms/step\n",
      "Epoch 65/300\n",
      "92/92 - 7s - loss: 3.3835 - accuracy: 0.2065 - 7s/epoch - 76ms/step\n",
      "Epoch 66/300\n",
      "92/92 - 7s - loss: 3.3483 - accuracy: 0.2121 - 7s/epoch - 71ms/step\n",
      "Epoch 67/300\n",
      "92/92 - 7s - loss: 3.3250 - accuracy: 0.2172 - 7s/epoch - 72ms/step\n",
      "Epoch 68/300\n",
      "92/92 - 7s - loss: 3.2902 - accuracy: 0.2169 - 7s/epoch - 72ms/step\n",
      "Epoch 69/300\n",
      "92/92 - 7s - loss: 3.2740 - accuracy: 0.2213 - 7s/epoch - 71ms/step\n",
      "Epoch 70/300\n",
      "92/92 - 7s - loss: 3.2319 - accuracy: 0.2305 - 7s/epoch - 72ms/step\n",
      "Epoch 71/300\n",
      "92/92 - 7s - loss: 3.2044 - accuracy: 0.2384 - 7s/epoch - 72ms/step\n",
      "Epoch 72/300\n",
      "92/92 - 7s - loss: 3.1705 - accuracy: 0.2406 - 7s/epoch - 72ms/step\n",
      "Epoch 73/300\n",
      "92/92 - 7s - loss: 3.1527 - accuracy: 0.2448 - 7s/epoch - 72ms/step\n",
      "Epoch 74/300\n",
      "92/92 - 7s - loss: 3.1263 - accuracy: 0.2450 - 7s/epoch - 71ms/step\n",
      "Epoch 75/300\n",
      "92/92 - 7s - loss: 3.1082 - accuracy: 0.2528 - 7s/epoch - 72ms/step\n",
      "Epoch 76/300\n",
      "92/92 - 7s - loss: 3.0779 - accuracy: 0.2589 - 7s/epoch - 71ms/step\n",
      "Epoch 77/300\n",
      "92/92 - 7s - loss: 3.0475 - accuracy: 0.2594 - 7s/epoch - 71ms/step\n",
      "Epoch 78/300\n",
      "92/92 - 7s - loss: 3.0230 - accuracy: 0.2681 - 7s/epoch - 72ms/step\n",
      "Epoch 79/300\n",
      "92/92 - 7s - loss: 3.0006 - accuracy: 0.2732 - 7s/epoch - 72ms/step\n",
      "Epoch 80/300\n",
      "92/92 - 7s - loss: 2.9791 - accuracy: 0.2762 - 7s/epoch - 71ms/step\n",
      "Epoch 81/300\n",
      "92/92 - 7s - loss: 2.9530 - accuracy: 0.2817 - 7s/epoch - 76ms/step\n",
      "Epoch 82/300\n",
      "92/92 - 7s - loss: 2.9379 - accuracy: 0.2810 - 7s/epoch - 73ms/step\n",
      "Epoch 83/300\n",
      "92/92 - 7s - loss: 2.9011 - accuracy: 0.2924 - 7s/epoch - 72ms/step\n",
      "Epoch 84/300\n",
      "92/92 - 7s - loss: 2.8809 - accuracy: 0.2980 - 7s/epoch - 74ms/step\n",
      "Epoch 85/300\n",
      "92/92 - 7s - loss: 2.8626 - accuracy: 0.3015 - 7s/epoch - 73ms/step\n",
      "Epoch 86/300\n",
      "92/92 - 7s - loss: 2.8380 - accuracy: 0.3039 - 7s/epoch - 73ms/step\n",
      "Epoch 87/300\n",
      "92/92 - 7s - loss: 2.8238 - accuracy: 0.3085 - 7s/epoch - 72ms/step\n",
      "Epoch 88/300\n",
      "92/92 - 7s - loss: 2.7923 - accuracy: 0.3128 - 7s/epoch - 73ms/step\n",
      "Epoch 89/300\n",
      "92/92 - 7s - loss: 2.7710 - accuracy: 0.3203 - 7s/epoch - 72ms/step\n",
      "Epoch 90/300\n",
      "92/92 - 7s - loss: 2.7560 - accuracy: 0.3179 - 7s/epoch - 72ms/step\n",
      "Epoch 91/300\n",
      "92/92 - 7s - loss: 2.7311 - accuracy: 0.3283 - 7s/epoch - 73ms/step\n",
      "Epoch 92/300\n",
      "92/92 - 7s - loss: 2.7164 - accuracy: 0.3281 - 7s/epoch - 74ms/step\n",
      "Epoch 93/300\n",
      "92/92 - 7s - loss: 2.7018 - accuracy: 0.3332 - 7s/epoch - 72ms/step\n",
      "Epoch 94/300\n",
      "92/92 - 7s - loss: 2.6708 - accuracy: 0.3406 - 7s/epoch - 72ms/step\n",
      "Epoch 95/300\n",
      "92/92 - 7s - loss: 2.6619 - accuracy: 0.3440 - 7s/epoch - 72ms/step\n",
      "Epoch 96/300\n",
      "92/92 - 7s - loss: 2.6550 - accuracy: 0.3420 - 7s/epoch - 73ms/step\n",
      "Epoch 97/300\n",
      "92/92 - 7s - loss: 2.6229 - accuracy: 0.3459 - 7s/epoch - 73ms/step\n",
      "Epoch 98/300\n",
      "92/92 - 7s - loss: 2.6066 - accuracy: 0.3514 - 7s/epoch - 72ms/step\n",
      "Epoch 99/300\n",
      "92/92 - 7s - loss: 2.5814 - accuracy: 0.3606 - 7s/epoch - 73ms/step\n",
      "Epoch 100/300\n",
      "92/92 - 7s - loss: 2.5649 - accuracy: 0.3644 - 7s/epoch - 74ms/step\n",
      "Epoch 101/300\n",
      "92/92 - 7s - loss: 2.8460 - accuracy: 0.3120 - 7s/epoch - 79ms/step\n",
      "Epoch 102/300\n",
      "92/92 - 7s - loss: 2.6441 - accuracy: 0.3394 - 7s/epoch - 78ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "92/92 - 7s - loss: 2.5754 - accuracy: 0.3632 - 7s/epoch - 78ms/step\n",
      "Epoch 104/300\n",
      "92/92 - 7s - loss: 2.5551 - accuracy: 0.3684 - 7s/epoch - 79ms/step\n",
      "Epoch 105/300\n",
      "92/92 - 7s - loss: 2.5301 - accuracy: 0.3717 - 7s/epoch - 79ms/step\n",
      "Epoch 106/300\n",
      "92/92 - 7s - loss: 2.5087 - accuracy: 0.3805 - 7s/epoch - 79ms/step\n",
      "Epoch 107/300\n",
      "92/92 - 7s - loss: 2.4995 - accuracy: 0.3814 - 7s/epoch - 79ms/step\n",
      "Epoch 108/300\n",
      "92/92 - 7s - loss: 2.4831 - accuracy: 0.3793 - 7s/epoch - 80ms/step\n",
      "Epoch 109/300\n",
      "92/92 - 8s - loss: 2.4592 - accuracy: 0.3865 - 8s/epoch - 82ms/step\n",
      "Epoch 110/300\n",
      "92/92 - 7s - loss: 2.4383 - accuracy: 0.3900 - 7s/epoch - 79ms/step\n",
      "Epoch 111/300\n",
      "92/92 - 7s - loss: 2.4281 - accuracy: 0.3964 - 7s/epoch - 78ms/step\n",
      "Epoch 112/300\n",
      "92/92 - 7s - loss: 2.4142 - accuracy: 0.4003 - 7s/epoch - 74ms/step\n",
      "Epoch 113/300\n",
      "92/92 - 7s - loss: 2.4038 - accuracy: 0.3960 - 7s/epoch - 74ms/step\n",
      "Epoch 114/300\n",
      "92/92 - 7s - loss: 2.3845 - accuracy: 0.4035 - 7s/epoch - 74ms/step\n",
      "Epoch 115/300\n",
      "92/92 - 7s - loss: 2.3688 - accuracy: 0.4065 - 7s/epoch - 74ms/step\n",
      "Epoch 116/300\n",
      "92/92 - 7s - loss: 2.3510 - accuracy: 0.4149 - 7s/epoch - 74ms/step\n",
      "Epoch 117/300\n",
      "92/92 - 7s - loss: 2.3364 - accuracy: 0.4128 - 7s/epoch - 76ms/step\n",
      "Epoch 118/300\n",
      "92/92 - 7s - loss: 2.3223 - accuracy: 0.4190 - 7s/epoch - 74ms/step\n",
      "Epoch 119/300\n",
      "92/92 - 7s - loss: 2.3145 - accuracy: 0.4207 - 7s/epoch - 75ms/step\n",
      "Epoch 120/300\n",
      "92/92 - 7s - loss: 2.2968 - accuracy: 0.4248 - 7s/epoch - 74ms/step\n",
      "Epoch 121/300\n",
      "92/92 - 7s - loss: 2.2807 - accuracy: 0.4266 - 7s/epoch - 74ms/step\n",
      "Epoch 122/300\n",
      "92/92 - 7s - loss: 2.2663 - accuracy: 0.4301 - 7s/epoch - 74ms/step\n",
      "Epoch 123/300\n",
      "92/92 - 7s - loss: 2.2542 - accuracy: 0.4268 - 7s/epoch - 74ms/step\n",
      "Epoch 124/300\n",
      "92/92 - 7s - loss: 2.2404 - accuracy: 0.4350 - 7s/epoch - 74ms/step\n",
      "Epoch 125/300\n",
      "92/92 - 7s - loss: 2.2243 - accuracy: 0.4385 - 7s/epoch - 74ms/step\n",
      "Epoch 126/300\n",
      "92/92 - 7s - loss: 2.2148 - accuracy: 0.4417 - 7s/epoch - 74ms/step\n",
      "Epoch 127/300\n",
      "92/92 - 7s - loss: 2.2022 - accuracy: 0.4391 - 7s/epoch - 74ms/step\n",
      "Epoch 128/300\n",
      "92/92 - 7s - loss: 2.1844 - accuracy: 0.4493 - 7s/epoch - 74ms/step\n",
      "Epoch 129/300\n",
      "92/92 - 7s - loss: 2.1739 - accuracy: 0.4520 - 7s/epoch - 74ms/step\n",
      "Epoch 130/300\n",
      "92/92 - 7s - loss: 2.1633 - accuracy: 0.4500 - 7s/epoch - 74ms/step\n",
      "Epoch 131/300\n",
      "92/92 - 7s - loss: 2.1483 - accuracy: 0.4537 - 7s/epoch - 75ms/step\n",
      "Epoch 132/300\n",
      "92/92 - 7s - loss: 2.1362 - accuracy: 0.4550 - 7s/epoch - 74ms/step\n",
      "Epoch 133/300\n",
      "92/92 - 7s - loss: 2.1207 - accuracy: 0.4602 - 7s/epoch - 75ms/step\n",
      "Epoch 134/300\n",
      "92/92 - 7s - loss: 2.1097 - accuracy: 0.4671 - 7s/epoch - 75ms/step\n",
      "Epoch 135/300\n",
      "92/92 - 7s - loss: 2.0920 - accuracy: 0.4712 - 7s/epoch - 74ms/step\n",
      "Epoch 136/300\n",
      "92/92 - 7s - loss: 2.0824 - accuracy: 0.4689 - 7s/epoch - 74ms/step\n",
      "Epoch 137/300\n",
      "92/92 - 7s - loss: 2.0745 - accuracy: 0.4681 - 7s/epoch - 75ms/step\n",
      "Epoch 138/300\n",
      "92/92 - 7s - loss: 2.0581 - accuracy: 0.4706 - 7s/epoch - 73ms/step\n",
      "Epoch 139/300\n",
      "92/92 - 7s - loss: 2.0433 - accuracy: 0.4807 - 7s/epoch - 73ms/step\n",
      "Epoch 140/300\n",
      "92/92 - 7s - loss: 2.0287 - accuracy: 0.4853 - 7s/epoch - 72ms/step\n",
      "Epoch 141/300\n",
      "92/92 - 7s - loss: 2.0137 - accuracy: 0.4865 - 7s/epoch - 72ms/step\n",
      "Epoch 142/300\n",
      "92/92 - 7s - loss: 2.0192 - accuracy: 0.4810 - 7s/epoch - 72ms/step\n",
      "Epoch 143/300\n",
      "92/92 - 7s - loss: 2.0028 - accuracy: 0.4868 - 7s/epoch - 73ms/step\n",
      "Epoch 144/300\n",
      "92/92 - 7s - loss: 1.9876 - accuracy: 0.4949 - 7s/epoch - 72ms/step\n",
      "Epoch 145/300\n",
      "92/92 - 7s - loss: 1.9860 - accuracy: 0.4882 - 7s/epoch - 73ms/step\n",
      "Epoch 146/300\n",
      "92/92 - 7s - loss: 1.9662 - accuracy: 0.4908 - 7s/epoch - 73ms/step\n",
      "Epoch 147/300\n",
      "92/92 - 7s - loss: 1.9582 - accuracy: 0.4906 - 7s/epoch - 72ms/step\n",
      "Epoch 148/300\n",
      "92/92 - 7s - loss: 1.9441 - accuracy: 0.4984 - 7s/epoch - 73ms/step\n",
      "Epoch 149/300\n",
      "92/92 - 7s - loss: 1.9270 - accuracy: 0.5000 - 7s/epoch - 75ms/step\n",
      "Epoch 150/300\n",
      "92/92 - 7s - loss: 1.9046 - accuracy: 0.5089 - 7s/epoch - 75ms/step\n",
      "Epoch 151/300\n",
      "92/92 - 7s - loss: 1.9043 - accuracy: 0.5068 - 7s/epoch - 75ms/step\n",
      "Epoch 152/300\n",
      "92/92 - 7s - loss: 1.8868 - accuracy: 0.5151 - 7s/epoch - 75ms/step\n",
      "Epoch 153/300\n",
      "92/92 - 7s - loss: 1.8818 - accuracy: 0.5147 - 7s/epoch - 73ms/step\n",
      "Epoch 154/300\n",
      "92/92 - 7s - loss: 1.8752 - accuracy: 0.5163 - 7s/epoch - 74ms/step\n",
      "Epoch 155/300\n",
      "92/92 - 7s - loss: 1.8568 - accuracy: 0.5191 - 7s/epoch - 74ms/step\n",
      "Epoch 156/300\n",
      "92/92 - 7s - loss: 1.8606 - accuracy: 0.5155 - 7s/epoch - 75ms/step\n",
      "Epoch 157/300\n",
      "92/92 - 7s - loss: 1.8431 - accuracy: 0.5259 - 7s/epoch - 76ms/step\n",
      "Epoch 158/300\n",
      "92/92 - 7s - loss: 1.8276 - accuracy: 0.5247 - 7s/epoch - 75ms/step\n",
      "Epoch 159/300\n",
      "92/92 - 7s - loss: 1.8190 - accuracy: 0.5273 - 7s/epoch - 74ms/step\n",
      "Epoch 160/300\n",
      "92/92 - 7s - loss: 1.8149 - accuracy: 0.5293 - 7s/epoch - 74ms/step\n",
      "Epoch 161/300\n",
      "92/92 - 7s - loss: 1.7910 - accuracy: 0.5356 - 7s/epoch - 74ms/step\n",
      "Epoch 162/300\n",
      "92/92 - 7s - loss: 1.7911 - accuracy: 0.5364 - 7s/epoch - 73ms/step\n",
      "Epoch 163/300\n",
      "92/92 - 7s - loss: 1.7738 - accuracy: 0.5368 - 7s/epoch - 74ms/step\n",
      "Epoch 164/300\n",
      "92/92 - 7s - loss: 1.7572 - accuracy: 0.5436 - 7s/epoch - 73ms/step\n",
      "Epoch 165/300\n",
      "92/92 - 7s - loss: 1.7537 - accuracy: 0.5435 - 7s/epoch - 74ms/step\n",
      "Epoch 166/300\n",
      "92/92 - 7s - loss: 1.7350 - accuracy: 0.5489 - 7s/epoch - 75ms/step\n",
      "Epoch 167/300\n",
      "92/92 - 7s - loss: 1.7229 - accuracy: 0.5517 - 7s/epoch - 74ms/step\n",
      "Epoch 168/300\n",
      "92/92 - 7s - loss: 1.7720 - accuracy: 0.5360 - 7s/epoch - 74ms/step\n",
      "Epoch 169/300\n",
      "92/92 - 7s - loss: 1.7627 - accuracy: 0.5349 - 7s/epoch - 74ms/step\n",
      "Epoch 170/300\n",
      "92/92 - 7s - loss: 1.7062 - accuracy: 0.5540 - 7s/epoch - 75ms/step\n",
      "Epoch 171/300\n",
      "92/92 - 7s - loss: 1.6944 - accuracy: 0.5565 - 7s/epoch - 73ms/step\n",
      "Epoch 172/300\n",
      "92/92 - 7s - loss: 1.6813 - accuracy: 0.5605 - 7s/epoch - 74ms/step\n",
      "Epoch 173/300\n",
      "92/92 - 7s - loss: 1.6619 - accuracy: 0.5658 - 7s/epoch - 75ms/step\n",
      "Epoch 174/300\n",
      "92/92 - 7s - loss: 1.6437 - accuracy: 0.5702 - 7s/epoch - 74ms/step\n",
      "Epoch 175/300\n",
      "92/92 - 7s - loss: 1.6359 - accuracy: 0.5740 - 7s/epoch - 73ms/step\n",
      "Epoch 176/300\n",
      "92/92 - 7s - loss: 1.6344 - accuracy: 0.5711 - 7s/epoch - 74ms/step\n",
      "Epoch 177/300\n",
      "92/92 - 7s - loss: 1.7574 - accuracy: 0.5339 - 7s/epoch - 75ms/step\n",
      "Epoch 178/300\n",
      "92/92 - 7s - loss: 1.7119 - accuracy: 0.5433 - 7s/epoch - 74ms/step\n",
      "Epoch 179/300\n",
      "92/92 - 7s - loss: 1.6427 - accuracy: 0.5662 - 7s/epoch - 74ms/step\n",
      "Epoch 180/300\n",
      "92/92 - 7s - loss: 1.6068 - accuracy: 0.5777 - 7s/epoch - 75ms/step\n",
      "Epoch 181/300\n",
      "92/92 - 7s - loss: 1.5989 - accuracy: 0.5788 - 7s/epoch - 75ms/step\n",
      "Epoch 182/300\n",
      "92/92 - 7s - loss: 1.5882 - accuracy: 0.5831 - 7s/epoch - 73ms/step\n",
      "Epoch 183/300\n",
      "92/92 - 7s - loss: 1.5904 - accuracy: 0.5827 - 7s/epoch - 74ms/step\n",
      "Epoch 184/300\n",
      "92/92 - 7s - loss: 1.5669 - accuracy: 0.5910 - 7s/epoch - 73ms/step\n",
      "Epoch 185/300\n",
      "92/92 - 7s - loss: 1.5578 - accuracy: 0.5894 - 7s/epoch - 73ms/step\n",
      "Epoch 186/300\n",
      "92/92 - 7s - loss: 1.5410 - accuracy: 0.5924 - 7s/epoch - 74ms/step\n",
      "Epoch 187/300\n",
      "92/92 - 7s - loss: 1.5222 - accuracy: 0.5992 - 7s/epoch - 74ms/step\n",
      "Epoch 188/300\n",
      "92/92 - 7s - loss: 1.5130 - accuracy: 0.6030 - 7s/epoch - 73ms/step\n",
      "Epoch 189/300\n",
      "92/92 - 7s - loss: 1.5081 - accuracy: 0.6043 - 7s/epoch - 73ms/step\n",
      "Epoch 190/300\n",
      "92/92 - 7s - loss: 1.4949 - accuracy: 0.6071 - 7s/epoch - 73ms/step\n",
      "Epoch 191/300\n",
      "92/92 - 7s - loss: 1.4921 - accuracy: 0.6066 - 7s/epoch - 73ms/step\n",
      "Epoch 192/300\n",
      "92/92 - 7s - loss: 1.4847 - accuracy: 0.6074 - 7s/epoch - 74ms/step\n",
      "Epoch 193/300\n",
      "92/92 - 7s - loss: 1.4747 - accuracy: 0.6069 - 7s/epoch - 73ms/step\n",
      "Epoch 194/300\n",
      "92/92 - 7s - loss: 1.4639 - accuracy: 0.6098 - 7s/epoch - 74ms/step\n",
      "Epoch 195/300\n",
      "92/92 - 7s - loss: 1.4559 - accuracy: 0.6147 - 7s/epoch - 73ms/step\n",
      "Epoch 196/300\n",
      "92/92 - 7s - loss: 1.4498 - accuracy: 0.6125 - 7s/epoch - 74ms/step\n",
      "Epoch 197/300\n",
      "92/92 - 7s - loss: 1.4370 - accuracy: 0.6180 - 7s/epoch - 74ms/step\n",
      "Epoch 198/300\n",
      "92/92 - 7s - loss: 1.4273 - accuracy: 0.6198 - 7s/epoch - 73ms/step\n",
      "Epoch 199/300\n",
      "92/92 - 7s - loss: 1.4105 - accuracy: 0.6280 - 7s/epoch - 74ms/step\n",
      "Epoch 200/300\n",
      "92/92 - 7s - loss: 1.3946 - accuracy: 0.6295 - 7s/epoch - 73ms/step\n",
      "Epoch 201/300\n",
      "92/92 - 7s - loss: 1.3889 - accuracy: 0.6321 - 7s/epoch - 73ms/step\n",
      "Epoch 202/300\n",
      "92/92 - 7s - loss: 1.3719 - accuracy: 0.6345 - 7s/epoch - 77ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/300\n",
      "92/92 - 7s - loss: 1.3725 - accuracy: 0.6362 - 7s/epoch - 73ms/step\n",
      "Epoch 204/300\n",
      "92/92 - 7s - loss: 1.3586 - accuracy: 0.6398 - 7s/epoch - 73ms/step\n",
      "Epoch 205/300\n",
      "92/92 - 7s - loss: 1.3503 - accuracy: 0.6427 - 7s/epoch - 75ms/step\n",
      "Epoch 206/300\n",
      "92/92 - 7s - loss: 1.3409 - accuracy: 0.6451 - 7s/epoch - 73ms/step\n",
      "Epoch 207/300\n",
      "92/92 - 7s - loss: 1.3298 - accuracy: 0.6459 - 7s/epoch - 74ms/step\n",
      "Epoch 208/300\n",
      "92/92 - 7s - loss: 1.3302 - accuracy: 0.6497 - 7s/epoch - 73ms/step\n",
      "Epoch 209/300\n",
      "92/92 - 7s - loss: 1.3268 - accuracy: 0.6459 - 7s/epoch - 73ms/step\n",
      "Epoch 210/300\n",
      "92/92 - 7s - loss: 1.3146 - accuracy: 0.6503 - 7s/epoch - 73ms/step\n",
      "Epoch 211/300\n",
      "92/92 - 7s - loss: 1.2957 - accuracy: 0.6551 - 7s/epoch - 73ms/step\n",
      "Epoch 212/300\n",
      "92/92 - 7s - loss: 1.2821 - accuracy: 0.6591 - 7s/epoch - 73ms/step\n",
      "Epoch 213/300\n",
      "92/92 - 7s - loss: 1.2816 - accuracy: 0.6590 - 7s/epoch - 73ms/step\n",
      "Epoch 214/300\n",
      "92/92 - 7s - loss: 1.2747 - accuracy: 0.6621 - 7s/epoch - 74ms/step\n",
      "Epoch 215/300\n",
      "92/92 - 7s - loss: 1.2644 - accuracy: 0.6667 - 7s/epoch - 73ms/step\n",
      "Epoch 216/300\n",
      "92/92 - 7s - loss: 1.2496 - accuracy: 0.6656 - 7s/epoch - 73ms/step\n",
      "Epoch 217/300\n",
      "92/92 - 7s - loss: 1.2411 - accuracy: 0.6672 - 7s/epoch - 72ms/step\n",
      "Epoch 218/300\n",
      "92/92 - 7s - loss: 1.2278 - accuracy: 0.6755 - 7s/epoch - 73ms/step\n",
      "Epoch 219/300\n",
      "92/92 - 7s - loss: 1.2177 - accuracy: 0.6777 - 7s/epoch - 73ms/step\n",
      "Epoch 220/300\n",
      "92/92 - 7s - loss: 1.2195 - accuracy: 0.6752 - 7s/epoch - 73ms/step\n",
      "Epoch 221/300\n",
      "92/92 - 7s - loss: 1.1974 - accuracy: 0.6826 - 7s/epoch - 73ms/step\n",
      "Epoch 222/300\n",
      "92/92 - 7s - loss: 1.1884 - accuracy: 0.6861 - 7s/epoch - 73ms/step\n",
      "Epoch 223/300\n",
      "92/92 - 7s - loss: 1.1921 - accuracy: 0.6816 - 7s/epoch - 73ms/step\n",
      "Epoch 224/300\n",
      "92/92 - 7s - loss: 1.1710 - accuracy: 0.6867 - 7s/epoch - 74ms/step\n",
      "Epoch 225/300\n",
      "92/92 - 7s - loss: 1.1766 - accuracy: 0.6860 - 7s/epoch - 74ms/step\n",
      "Epoch 226/300\n",
      "92/92 - 7s - loss: 1.1602 - accuracy: 0.6926 - 7s/epoch - 72ms/step\n",
      "Epoch 227/300\n",
      "92/92 - 7s - loss: 1.1447 - accuracy: 0.6938 - 7s/epoch - 73ms/step\n",
      "Epoch 228/300\n",
      "92/92 - 7s - loss: 1.1441 - accuracy: 0.6930 - 7s/epoch - 73ms/step\n",
      "Epoch 229/300\n",
      "92/92 - 7s - loss: 1.1380 - accuracy: 0.6956 - 7s/epoch - 73ms/step\n",
      "Epoch 230/300\n",
      "92/92 - 7s - loss: 1.1266 - accuracy: 0.7013 - 7s/epoch - 72ms/step\n",
      "Epoch 231/300\n",
      "92/92 - 7s - loss: 1.1105 - accuracy: 0.7030 - 7s/epoch - 73ms/step\n",
      "Epoch 232/300\n",
      "92/92 - 7s - loss: 1.1096 - accuracy: 0.7051 - 7s/epoch - 73ms/step\n",
      "Epoch 233/300\n",
      "92/92 - 7s - loss: 1.1010 - accuracy: 0.7039 - 7s/epoch - 72ms/step\n",
      "Epoch 234/300\n",
      "92/92 - 7s - loss: 1.0899 - accuracy: 0.7108 - 7s/epoch - 73ms/step\n",
      "Epoch 235/300\n",
      "92/92 - 7s - loss: 1.0764 - accuracy: 0.7138 - 7s/epoch - 73ms/step\n",
      "Epoch 236/300\n",
      "92/92 - 7s - loss: 1.0721 - accuracy: 0.7130 - 7s/epoch - 74ms/step\n",
      "Epoch 237/300\n",
      "92/92 - 7s - loss: 1.0519 - accuracy: 0.7241 - 7s/epoch - 73ms/step\n",
      "Epoch 238/300\n",
      "92/92 - 7s - loss: 1.0451 - accuracy: 0.7248 - 7s/epoch - 74ms/step\n",
      "Epoch 239/300\n",
      "92/92 - 7s - loss: 1.0385 - accuracy: 0.7231 - 7s/epoch - 79ms/step\n",
      "Epoch 240/300\n",
      "92/92 - 7s - loss: 1.0305 - accuracy: 0.7249 - 7s/epoch - 78ms/step\n",
      "Epoch 241/300\n",
      "92/92 - 7s - loss: 1.0314 - accuracy: 0.7278 - 7s/epoch - 80ms/step\n",
      "Epoch 242/300\n",
      "92/92 - 7s - loss: 1.0158 - accuracy: 0.7302 - 7s/epoch - 79ms/step\n",
      "Epoch 243/300\n",
      "92/92 - 7s - loss: 1.0175 - accuracy: 0.7287 - 7s/epoch - 77ms/step\n",
      "Epoch 244/300\n",
      "92/92 - 7s - loss: 1.0062 - accuracy: 0.7331 - 7s/epoch - 78ms/step\n",
      "Epoch 245/300\n",
      "92/92 - 7s - loss: 0.9889 - accuracy: 0.7390 - 7s/epoch - 74ms/step\n",
      "Epoch 246/300\n",
      "92/92 - 7s - loss: 0.9824 - accuracy: 0.7391 - 7s/epoch - 73ms/step\n",
      "Epoch 247/300\n",
      "92/92 - 7s - loss: 0.9834 - accuracy: 0.7375 - 7s/epoch - 73ms/step\n",
      "Epoch 248/300\n",
      "92/92 - 7s - loss: 0.9712 - accuracy: 0.7437 - 7s/epoch - 74ms/step\n",
      "Epoch 249/300\n",
      "92/92 - 7s - loss: 0.9662 - accuracy: 0.7441 - 7s/epoch - 72ms/step\n",
      "Epoch 250/300\n",
      "92/92 - 7s - loss: 0.9666 - accuracy: 0.7414 - 7s/epoch - 72ms/step\n",
      "Epoch 251/300\n",
      "92/92 - 7s - loss: 0.9543 - accuracy: 0.7488 - 7s/epoch - 74ms/step\n",
      "Epoch 252/300\n",
      "92/92 - 7s - loss: 0.9369 - accuracy: 0.7531 - 7s/epoch - 73ms/step\n",
      "Epoch 253/300\n",
      "92/92 - 7s - loss: 0.9270 - accuracy: 0.7559 - 7s/epoch - 73ms/step\n",
      "Epoch 254/300\n",
      "92/92 - 7s - loss: 0.9262 - accuracy: 0.7562 - 7s/epoch - 73ms/step\n",
      "Epoch 255/300\n",
      "92/92 - 7s - loss: 0.9151 - accuracy: 0.7611 - 7s/epoch - 72ms/step\n",
      "Epoch 256/300\n",
      "92/92 - 7s - loss: 0.9010 - accuracy: 0.7620 - 7s/epoch - 73ms/step\n",
      "Epoch 257/300\n",
      "92/92 - 7s - loss: 0.8999 - accuracy: 0.7632 - 7s/epoch - 73ms/step\n",
      "Epoch 258/300\n",
      "92/92 - 7s - loss: 0.8894 - accuracy: 0.7701 - 7s/epoch - 74ms/step\n",
      "Epoch 259/300\n",
      "92/92 - 7s - loss: 0.8804 - accuracy: 0.7689 - 7s/epoch - 73ms/step\n",
      "Epoch 260/300\n",
      "92/92 - 7s - loss: 0.8774 - accuracy: 0.7729 - 7s/epoch - 72ms/step\n",
      "Epoch 261/300\n",
      "92/92 - 7s - loss: 0.8763 - accuracy: 0.7693 - 7s/epoch - 72ms/step\n",
      "Epoch 262/300\n",
      "92/92 - 7s - loss: 0.8808 - accuracy: 0.7668 - 7s/epoch - 73ms/step\n",
      "Epoch 263/300\n",
      "92/92 - 7s - loss: 0.8795 - accuracy: 0.7665 - 7s/epoch - 73ms/step\n",
      "Epoch 264/300\n",
      "92/92 - 7s - loss: 0.8577 - accuracy: 0.7730 - 7s/epoch - 73ms/step\n",
      "Epoch 265/300\n",
      "92/92 - 7s - loss: 0.8424 - accuracy: 0.7776 - 7s/epoch - 73ms/step\n",
      "Epoch 266/300\n",
      "92/92 - 7s - loss: 0.8180 - accuracy: 0.7898 - 7s/epoch - 75ms/step\n",
      "Epoch 267/300\n",
      "92/92 - 7s - loss: 0.8210 - accuracy: 0.7877 - 7s/epoch - 73ms/step\n",
      "Epoch 268/300\n",
      "92/92 - 7s - loss: 0.8187 - accuracy: 0.7853 - 7s/epoch - 72ms/step\n",
      "Epoch 269/300\n",
      "92/92 - 7s - loss: 0.8107 - accuracy: 0.7883 - 7s/epoch - 75ms/step\n",
      "Epoch 270/300\n",
      "92/92 - 7s - loss: 0.7962 - accuracy: 0.7959 - 7s/epoch - 73ms/step\n",
      "Epoch 271/300\n",
      "92/92 - 7s - loss: 0.7843 - accuracy: 0.7959 - 7s/epoch - 73ms/step\n",
      "Epoch 272/300\n",
      "92/92 - 7s - loss: 0.7904 - accuracy: 0.7942 - 7s/epoch - 74ms/step\n",
      "Epoch 273/300\n",
      "92/92 - 7s - loss: 0.7773 - accuracy: 0.7983 - 7s/epoch - 74ms/step\n",
      "Epoch 274/300\n",
      "92/92 - 7s - loss: 0.7665 - accuracy: 0.7995 - 7s/epoch - 73ms/step\n",
      "Epoch 275/300\n",
      "92/92 - 7s - loss: 0.7567 - accuracy: 0.8075 - 7s/epoch - 73ms/step\n",
      "Epoch 276/300\n",
      "92/92 - 7s - loss: 0.7640 - accuracy: 0.8005 - 7s/epoch - 73ms/step\n",
      "Epoch 277/300\n",
      "92/92 - 7s - loss: 0.7580 - accuracy: 0.8028 - 7s/epoch - 73ms/step\n",
      "Epoch 278/300\n",
      "92/92 - 7s - loss: 0.7691 - accuracy: 0.7995 - 7s/epoch - 72ms/step\n",
      "Epoch 279/300\n",
      "92/92 - 7s - loss: 0.7422 - accuracy: 0.8078 - 7s/epoch - 74ms/step\n",
      "Epoch 280/300\n",
      "92/92 - 7s - loss: 0.7303 - accuracy: 0.8125 - 7s/epoch - 74ms/step\n",
      "Epoch 281/300\n",
      "92/92 - 7s - loss: 0.7155 - accuracy: 0.8148 - 7s/epoch - 73ms/step\n",
      "Epoch 282/300\n",
      "92/92 - 7s - loss: 0.7027 - accuracy: 0.8231 - 7s/epoch - 73ms/step\n",
      "Epoch 283/300\n",
      "92/92 - 7s - loss: 0.7002 - accuracy: 0.8220 - 7s/epoch - 74ms/step\n",
      "Epoch 284/300\n",
      "92/92 - 7s - loss: 0.6911 - accuracy: 0.8278 - 7s/epoch - 73ms/step\n",
      "Epoch 285/300\n",
      "92/92 - 7s - loss: 0.6822 - accuracy: 0.8271 - 7s/epoch - 73ms/step\n",
      "Epoch 286/300\n",
      "92/92 - 7s - loss: 0.6834 - accuracy: 0.8241 - 7s/epoch - 73ms/step\n",
      "Epoch 287/300\n",
      "92/92 - 7s - loss: 0.6836 - accuracy: 0.8245 - 7s/epoch - 73ms/step\n",
      "Epoch 288/300\n",
      "92/92 - 7s - loss: 0.6841 - accuracy: 0.8281 - 7s/epoch - 73ms/step\n",
      "Epoch 289/300\n",
      "92/92 - 7s - loss: 0.6708 - accuracy: 0.8312 - 7s/epoch - 73ms/step\n",
      "Epoch 290/300\n",
      "92/92 - 7s - loss: 0.6534 - accuracy: 0.8371 - 7s/epoch - 74ms/step\n",
      "Epoch 291/300\n",
      "92/92 - 7s - loss: 0.6489 - accuracy: 0.8388 - 7s/epoch - 73ms/step\n",
      "Epoch 292/300\n",
      "92/92 - 7s - loss: 0.6525 - accuracy: 0.8329 - 7s/epoch - 74ms/step\n",
      "Epoch 293/300\n",
      "92/92 - 7s - loss: 0.6295 - accuracy: 0.8447 - 7s/epoch - 73ms/step\n",
      "Epoch 294/300\n",
      "92/92 - 7s - loss: 0.6173 - accuracy: 0.8500 - 7s/epoch - 74ms/step\n",
      "Epoch 295/300\n",
      "92/92 - 7s - loss: 0.6191 - accuracy: 0.8484 - 7s/epoch - 73ms/step\n",
      "Epoch 296/300\n",
      "92/92 - 7s - loss: 0.6152 - accuracy: 0.8493 - 7s/epoch - 74ms/step\n",
      "Epoch 297/300\n",
      "92/92 - 7s - loss: 0.6165 - accuracy: 0.8480 - 7s/epoch - 74ms/step\n",
      "Epoch 298/300\n",
      "92/92 - 7s - loss: 0.6163 - accuracy: 0.8441 - 7s/epoch - 73ms/step\n",
      "Epoch 299/300\n",
      "92/92 - 7s - loss: 0.5949 - accuracy: 0.8526 - 7s/epoch - 74ms/step\n",
      "Epoch 300/300\n",
      "92/92 - 7s - loss: 0.5941 - accuracy: 0.8522 - 7s/epoch - 73ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2424f930310>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=300, verbose=2) \n",
    "# batch_size is number of sequences passed in at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "657d55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_moby_model.h5')\n",
    "dump(tokenizer, open('my_moby_tokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b6465e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    # best to have seq_len the same as the length of seed_text\n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0] # encode to be a sequence\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen = seq_len, truncating='pre') \n",
    "        # ensure it is 25 words by getting the last 25 words (cut off text or pad if needed)\n",
    "        # last 25 words so that we use the predicted words to predict the next (we add word to input_text)\n",
    "        \n",
    "        pred_word_ind = np.argmax(model.predict(pad_encoded), axis=-1)[0] # assign prob to next word and get highest prob\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        \n",
    "        input_text += ' ' + pred_word\n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1230056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "especially whenever my hypos get such an upper hand of me that it requires a strong moral principle to prevent me from deliberately stepping into the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = ' '.join(text_seq[100])\n",
    "print(seed_text)\n",
    "len(text_seq[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa8f9b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"street and methodically knocking people 's hats off then completely fascinated my attention and convinced me that how the plane iron came bump against me\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, seed_text, num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a846fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df61e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ebe15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b83f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8562ebb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"street and methodically knocking people 's hats off with ye far when thou shalt sit it up for whatever silver all this that i do\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('epochBIG.h5')\n",
    "tokenizer = load(open('epochBIG','rb'))\n",
    "generate_text(model, tokenizer, seq_len, seed_text, num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af6260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
